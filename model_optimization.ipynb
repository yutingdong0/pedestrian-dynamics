{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dataset_index = 1\n",
    "data_mapped = pickle.load(open(f'cleaned_data/data_{dataset_index}.pkl', \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pedestrians = data_mapped['id'].nunique()\n",
    "circumference = 2*np.pi*3 + 4 * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "\n",
    "pedestrian_ids = data_mapped['id'].unique()\n",
    "num_frames = data_mapped['frame'].nunique()\n",
    "\n",
    "# Map pedestrian ids to row indices\n",
    "id_to_index = {pid: i for i, pid in enumerate(pedestrian_ids)}\n",
    "\n",
    "real_positions = np.full((num_pedestrians, num_frames), np.nan)\n",
    "for _, row in data_mapped.iterrows():\n",
    "    pid_index = id_to_index[row['id']]\n",
    "    frame_index = int(row['frame']) - 1\n",
    "    real_positions[pid_index, frame_index] = row['position']\n",
    "\n",
    "t_eval = np.arange(num_frames)  # Frame indices = time steps\n",
    "# For testing\n",
    "# t_eval = np.linspace(0, num_frames - 1, num_frames)\n",
    "\n",
    "initial_positions = data_mapped.groupby('id').first()['position'].values\n",
    "\n",
    "# Original time axis (frames 0 to 1820)\n",
    "t_original = np.arange(real_positions.shape[1])\n",
    "\n",
    "# Interpolate real_positions over new t_eval\n",
    "interpolated_real_positions = np.array([\n",
    "    interp1d(t_original, real_positions[i], kind='linear')(t_eval)\n",
    "    for i in range(real_positions.shape[0])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pedestrian_ode(t, l, c, s):\n",
    "    # Default circumference\n",
    "    circumference = 2*np.pi*3 + 4 * 2\n",
    "\n",
    "    n = len(l)  # Number of pedestrians\n",
    "    dl_dt = np.zeros(n)\n",
    "    \n",
    "    for i in range(n): \n",
    "        if i == n - 1:\n",
    "            gap = l[0] - (l[i] - circumference)\n",
    "        else:\n",
    "            gap = l[i + 1] - l[i]\n",
    "        if gap > s: \n",
    "            dl_dt[i] = c * (1 - s / gap)\n",
    "        else:\n",
    "            dl_dt[i] = 0  # If too close, pedestrian stops\n",
    "    \n",
    "    return dl_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_order_pedestrian_ode(t, y, c, s, tau=0.5):\n",
    "    \"\"\"\n",
    "    Second-order pedestrian model:\n",
    "    y = [x_0, ..., x_n, u_0, ..., u_n]  (positions + velocities)\n",
    "    \"\"\"\n",
    "    # Default circumference\n",
    "    circumference = 2*np.pi*3 + 4 * 2\n",
    "\n",
    "    n = len(y) // 2  # Number of pedestrians\n",
    "    x = y[:n]  # Positions\n",
    "    u = y[n:]  # Velocities\n",
    "\n",
    "    dxdt = u  # dx/dt = velocity\n",
    "    dudt = np.zeros(n)  # Initialize acceleration\n",
    "    \n",
    "    for i in range(n): \n",
    "        next_index = (i + 1) % n  # Wrap around for circular track\n",
    "        gap = (x[next_index] - x[i]) % circumference  # Circular gap\n",
    "        if gap > s:\n",
    "            desired_velocity = c * (1 - s / gap)  # Desired velocity based on gap\n",
    "            dudt[i] = (desired_velocity - u[i]) / tau  # Acceleration towards desired velocity\n",
    "        else:\n",
    "            dudt[i] = -u[i] / tau  # Deceleration if too close\n",
    "        \n",
    "    return np.concatenate([dxdt, dudt])  # Return both position and velocity derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_predicted_positions(params, t_eval):\n",
    "    # Solve ODE\n",
    "    c_values = params[0]\n",
    "    s_value = params[-1]\n",
    "    sol = solve_ivp(\n",
    "        # Use the second-order ODE function\n",
    "        pedestrian_ode, \n",
    "        (0, t_eval[-1]), \n",
    "        initial_positions, \n",
    "        args=(c_values, s_value), \n",
    "        t_eval=t_eval\n",
    "    )\n",
    "    \n",
    "    # Compute predicted positions\n",
    "    predicted_positions = sol.y % circumference\n",
    "    return predicted_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(params, real_positions, epsilon=1e-8, t_eval=t_eval):\n",
    "    grad = np.zeros_like(params)\n",
    "    for i in range(len(params)):\n",
    "        delta = np.zeros_like(params)\n",
    "        delta[i] = epsilon\n",
    "        predicted_positions = compute_predicted_positions(params - delta, t_eval)\n",
    "        predicted_positions_delta = compute_predicted_positions(params + delta, t_eval)\n",
    "        \n",
    "        grad[i] = (loss_function(real_positions, predicted_positions_delta, t_eval)\n",
    "                    - loss_function(real_positions, predicted_positions, t_eval)) / (2 * epsilon)\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient in iteration0: [143392.15690597 -22363.81960756], params: [0.08, 0.04]\n",
      "learning_rate: 6.973882125615283e-11\n",
      "Iteration 1/1000: Loss = 4497.971963104515\n",
      "gradient in iteration1: [-722927.81742362  -46932.85322901], params: [0.07999    0.04000156]\n",
      "learning_rate: 1.383263966192099e-11\n",
      "Iteration 2/1000: Loss = 4505.868907648371\n",
      "gradient in iteration2: [ 35059.13177833 -18082.71158552], params: [0.08       0.04000221]\n",
      "learning_rate: 2.852323914701807e-10\n",
      "Iteration 3/1000: Loss = 4497.9283630202235\n",
      "gradient in iteration3: [-757923.02700393  -58268.28873978], params: [0.07999    0.04000737]\n",
      "learning_rate: 1.3193951949883428e-11\n",
      "Iteration 4/1000: Loss = 4505.561935753396\n",
      "gradient in iteration4: [-15772.15539328 -17732.78313522], params: [0.08       0.04000814]\n",
      "learning_rate: 5.63927270961816e-10\n",
      "Iteration 5/1000: Loss = 4497.824589603423\n",
      "gradient in iteration5: [4526891.26218502 -777150.64453514], params: [0.08000889 0.04001814]\n",
      "learning_rate: 2.209021472093688e-12\n",
      "Iteration 6/1000: Loss = 4502.1903124942355\n",
      "gradient in iteration6: [-241434.03788912  -25773.42092991], params: [0.07999889 0.04001985]\n",
      "learning_rate: 4.141918052413353e-11\n",
      "Iteration 7/1000: Loss = 4497.754170244749\n",
      "gradient in iteration7: [95558862.98221594 11183122.00028413], params: [0.08000889 0.04002092]\n",
      "learning_rate: 1.0464754066675175e-13\n",
      "Iteration 8/1000: Loss = 4498.002322445069\n",
      "gradient in iteration8: [-241215.26143972  -25739.99877313], params: [0.07999889 0.04001975]\n",
      "learning_rate: 4.145674672619755e-11\n",
      "Iteration 9/1000: Loss = 4497.756817321386\n",
      "gradient in iteration9: [ 1.99623742e+08 -2.04508463e+07], params: [0.08000889 0.04002082]\n",
      "learning_rate: 5.009424169734865e-14\n",
      "Iteration 10/1000: Loss = 4503.2369607746205\n",
      "gradient in iteration10: [-245480.44971198  -26380.50104906], params: [0.07999889 0.04002184]\n",
      "learning_rate: 4.07364415851977e-11\n",
      "Iteration 11/1000: Loss = 4497.702295190856\n",
      "gradient in iteration11: [ 81468530.58351552 -16720967.14937652], params: [0.08000889 0.04002292]\n",
      "learning_rate: 1.2274678244931323e-13\n",
      "Iteration 12/1000: Loss = 4498.935145504384\n",
      "gradient in iteration12: [-249414.08196355  -27112.46684157], params: [0.07999889 0.04002497]\n",
      "learning_rate: 4.00939671139395e-11\n",
      "Iteration 13/1000: Loss = 4497.618565367908\n",
      "gradient in iteration13: [-1800863.18514441   123801.9300672 ], params: [0.08000889 0.04002606]\n",
      "learning_rate: 5.552892680849667e-12\n",
      "Iteration 14/1000: Loss = 4501.295531968106\n",
      "gradient in iteration14: [-14406411.26846458    101681.10884479], params: [0.08001889 0.04002537]\n",
      "learning_rate: 6.94135396640373e-13\n",
      "Iteration 15/1000: Loss = 4500.9297038484665\n",
      "gradient in iteration15: [-4404059.54814196  7602525.01586365], params: [0.08002889 0.0400253 ]\n",
      "learning_rate: 1.3153524623902865e-12\n",
      "Iteration 16/1000: Loss = 4485.395155709431\n",
      "gradient in iteration16: [-5.30604131e+08  3.66156778e+07], params: [0.08003469 0.0400153 ]\n",
      "learning_rate: 1.8846442026873244e-14\n",
      "Iteration 17/1000: Loss = 4502.942576527633\n",
      "gradient in iteration17: [ 2.47138569e+08 -8.80732614e+02], params: [0.08004469 0.04001461]\n",
      "learning_rate: 4.0463129795147127e-14\n",
      "Iteration 18/1000: Loss = 4487.662862717219\n",
      "gradient in iteration18: [-3.73401936e+08 -8.02271687e+08], params: [0.08003469 0.04001461]\n",
      "learning_rate: 1.2464605400995112e-14\n",
      "Iteration 19/1000: Loss = 4482.609232374827\n",
      "gradient in iteration19: [-1.21094778e+08  2.02314246e+06], params: [0.08003934 0.04002461]\n",
      "learning_rate: 8.25799443710535e-14\n",
      "Iteration 20/1000: Loss = 4503.020292771646\n",
      "gradient in iteration20: [-66704163.87037221   2856424.14696667], params: [0.08004934 0.04002444]\n",
      "learning_rate: 1.4991567871884637e-13\n",
      "Iteration 21/1000: Loss = 4510.689668898142\n",
      "gradient in iteration21: [-1.32129306e+08  1.89285302e+04], params: [0.08005934 0.04002401]\n",
      "learning_rate: 7.56834370449451e-14\n",
      "Iteration 22/1000: Loss = 4501.687389233081\n",
      "gradient in iteration22: [-6365190.45715431   135495.38807638], params: [0.08006934 0.04002401]\n",
      "learning_rate: 1.5710448991766232e-12\n",
      "Iteration 23/1000: Loss = 4495.607028654983\n",
      "gradient in iteration23: [-2201606.87308635   146691.38727186], params: [0.08007934 0.0400238 ]\n",
      "learning_rate: 4.542136982876232e-12\n",
      "Iteration 24/1000: Loss = 4529.024446399723\n",
      "gradient in iteration24: [42342518.12554248   -88005.97274785], params: [0.08008934 0.04002313]\n",
      "learning_rate: 2.3616923231515726e-13\n",
      "Iteration 25/1000: Loss = 4517.909107582941\n",
      "gradient in iteration25: [-2236752.89244056   139333.75257693], params: [0.08007934 0.04002315]\n",
      "learning_rate: 4.470766544572929e-12\n",
      "Iteration 26/1000: Loss = 4528.932314858323\n",
      "gradient in iteration26: [ 4315519.75925686 18838905.59538486], params: [0.08008934 0.04002253]\n",
      "learning_rate: 5.308163974477261e-13\n",
      "Iteration 27/1000: Loss = 4519.07321746841\n",
      "gradient in iteration27: [-3290489.46082366  -102933.78513779], params: [0.08008705 0.04001253]\n",
      "learning_rate: 3.039061549674994e-12\n",
      "Iteration 28/1000: Loss = 4514.110142215072\n",
      "gradient in iteration28: [5082927.22547594  -13151.70793532], params: [0.08009705 0.04001284]\n",
      "learning_rate: 1.967370288104734e-12\n",
      "Iteration 29/1000: Loss = 4513.359355703333\n",
      "gradient in iteration29: [-4406465.20819428  -132206.32263256], params: [0.08008705 0.04001287]\n",
      "learning_rate: 2.2693927053829835e-12\n",
      "Iteration 30/1000: Loss = 4514.06972134135\n",
      "gradient in iteration30: [5090338.83481607  -13160.94221693], params: [0.08009705 0.04001317]\n",
      "learning_rate: 1.9645057675932374e-12\n",
      "Iteration 31/1000: Loss = 4513.355068027561\n",
      "gradient in iteration31: [-5602251.26250771  -149047.02766216], params: [0.08008705 0.04001319]\n",
      "learning_rate: 1.7849966971177498e-12\n",
      "Iteration 32/1000: Loss = 4514.024263146964\n",
      "gradient in iteration32: [5096983.53594104  -13169.16873293], params: [0.08009705 0.04001346]\n",
      "learning_rate: 1.9619447325041715e-12\n",
      "Iteration 33/1000: Loss = 4513.351225108098\n",
      "gradient in iteration33: [-6838256.91644415  -176533.63561294], params: [0.08008705 0.04001349]\n",
      "learning_rate: 1.4623609674495725e-12\n",
      "Iteration 34/1000: Loss = 4513.977049105185\n",
      "gradient in iteration34: [5103453.94664     -13177.15496043], params: [0.08009705 0.04001374]\n",
      "learning_rate: 1.9594572821772546e-12\n",
      "Iteration 35/1000: Loss = 4513.347484021563\n",
      "gradient in iteration35: [-8087548.77034517  -217492.59890385], params: [0.08008705 0.04001377]\n",
      "learning_rate: 1.2364685869552061e-12\n",
      "Iteration 36/1000: Loss = 4513.921505580644\n",
      "gradient in iteration36: [5110175.19356756  -13185.41194451], params: [0.08009705 0.04001404]\n",
      "learning_rate: 1.9568800718588908e-12\n",
      "Iteration 37/1000: Loss = 4513.343598928298\n",
      "gradient in iteration37: [-9762698.04032199  -283519.10436868], params: [0.08008705 0.04001406]\n",
      "learning_rate: 1.024307005983172e-12\n",
      "Iteration 38/1000: Loss = 4513.848429049885\n",
      "gradient in iteration38: [5117392.72142222  -13194.29134128], params: [0.08009705 0.04001435]\n",
      "learning_rate: 1.954120104587324e-12\n",
      "Iteration 39/1000: Loss = 4513.339428124609\n",
      "gradient in iteration39: [-11545086.67655136   -270259.65900975], params: [0.08008705 0.04001438]\n",
      "learning_rate: 8.661693307431369e-13\n",
      "Iteration 40/1000: Loss = 4513.742478513875\n",
      "gradient in iteration40: [5123329.46582319  -13201.51986874], params: [0.08009705 0.04001461]\n",
      "learning_rate: 1.9518557349684817e-12\n",
      "Iteration 41/1000: Loss = 4513.33599833101\n",
      "gradient in iteration41: [-8346828.26052813   -21251.85769728], params: [0.08008705 0.04001464]\n",
      "learning_rate: 1.198059872309781e-12\n",
      "Iteration 42/1000: Loss = 4513.688497054115\n",
      "gradient in iteration42: [5124500.37726921  -13202.96883023], params: [0.08009705 0.04001467]\n",
      "learning_rate: 1.951409749984033e-12\n",
      "Iteration 43/1000: Loss = 4513.335321999886\n",
      "gradient in iteration43: [-7705814.15968081   -20600.76230919], params: [0.08008705 0.04001469]\n",
      "learning_rate: 1.2977214078588962e-12\n",
      "Iteration 44/1000: Loss = 4513.68742507984\n",
      "gradient in iteration44: [5125700.40259561  -13204.46067439], params: [0.08009705 0.04001472]\n",
      "learning_rate: 1.9509528873236687e-12\n",
      "Iteration 45/1000: Loss = 4513.334628825625\n",
      "gradient in iteration45: [-7019672.54353804   -19929.50410568], params: [0.08008705 0.04001474]\n",
      "learning_rate: 1.4245678752074422e-12\n",
      "Iteration 46/1000: Loss = 4513.686361237051\n",
      "gradient in iteration46: [5126938.47103947  -13205.9396492 ], params: [0.08009705 0.04001477]\n",
      "learning_rate: 1.9504817653824003e-12\n",
      "Iteration 47/1000: Loss = 4513.3339137355315\n",
      "gradient in iteration47: [-7167846.74873452   -19232.5088392 ], params: [0.08008705 0.0400148 ]\n",
      "learning_rate: 1.3951191132491077e-12\n",
      "Iteration 48/1000: Loss = 4513.685300924354\n",
      "gradient in iteration48: [5128140.97469345  -13207.43399447], params: [0.08009705 0.04001482]\n",
      "learning_rate: 1.9500243946779913e-12\n",
      "Iteration 49/1000: Loss = 4513.333219202065\n",
      "gradient in iteration49: [-7332444.42444629   -18551.19812717], params: [0.08008705 0.04001485]\n",
      "learning_rate: 1.3638016766496183e-12\n",
      "Iteration 50/1000: Loss = 4513.684307445987\n",
      "gradient in iteration50: [5129308.58267282  -13208.83916378], params: [0.08009705 0.04001488]\n",
      "learning_rate: 1.949580501703628e-12\n",
      "Iteration 51/1000: Loss = 4513.332544861874\n",
      "gradient in iteration51: [-7491139.06813363   -17885.19648471], params: [0.08008705 0.0400149 ]\n",
      "learning_rate: 1.334910473433706e-12\n",
      "Iteration 52/1000: Loss = 4513.683377351516\n",
      "gradient in iteration52: [5130443.76248217  -13210.20163232], params: [0.08009705 0.04001493]\n",
      "learning_rate: 1.9491491307492424e-12\n",
      "Iteration 53/1000: Loss = 4513.331889315558\n",
      "gradient in iteration53: [-7644696.10994638   -17233.53789203], params: [0.08008705 0.04001495]\n",
      "learning_rate: 1.3080964705698604e-12\n",
      "Iteration 54/1000: Loss = 4513.6825059693365\n",
      "gradient in iteration54: [5131548.49636521  -13211.56787526], params: [0.08009705 0.04001497]\n",
      "learning_rate: 1.9487295125600425e-12\n",
      "Iteration 55/1000: Loss = 4513.331251338298\n",
      "gradient in iteration55: [-7793718.18225627   -16595.14532548], params: [0.08008705 0.040015  ]\n",
      "learning_rate: 1.283084628690669e-12\n",
      "Iteration 56/1000: Loss = 4513.681689174948\n",
      "gradient in iteration56: [5132624.72477436  -13212.86044913], params: [0.08009705 0.04001502]\n",
      "learning_rate: 1.9483208954926315e-12\n",
      "Iteration 57/1000: Loss = 4513.330629852792\n",
      "gradient in iteration57: [-7938138.45093609   -16991.91366242], params: [0.08008705 0.04001505]\n",
      "learning_rate: 1.2597411927000044e-12\n",
      "Iteration 58/1000: Loss = 4513.680903221263\n",
      "gradient in iteration58: [5133703.64096772  -13214.18044427], params: [0.08009705 0.04001507]\n",
      "learning_rate: 1.947911429907738e-12\n",
      "Iteration 59/1000: Loss = 4513.330006857566\n",
      "gradient in iteration59: [-8082563.42329514   -16398.8010172 ], params: [0.08008705 0.04001509]\n",
      "learning_rate: 1.237231244134568e-12\n",
      "Iteration 60/1000: Loss = 4513.680116100851\n",
      "gradient in iteration60: [5134757.08225997  -13215.44173152], params: [0.08009705 0.04001511]\n",
      "learning_rate: 1.947511798474153e-12\n",
      "Iteration 61/1000: Loss = 4513.329398591179\n",
      "gradient in iteration61: [-8223595.33058261   -15817.1566909 ], params: [0.08008705 0.04001514]\n",
      "learning_rate: 1.2160131424282447e-12\n",
      "Iteration 62/1000: Loss = 4513.679374699238\n",
      "gradient in iteration62: [5135786.42344328  -13216.69965364], params: [0.08009705 0.04001516]\n",
      "learning_rate: 1.9471214679709208e-12\n",
      "Iteration 63/1000: Loss = 4513.328804249833\n",
      "gradient in iteration63: [-8353162.64174253   -15246.55085632], params: [0.08008705 0.04001518]\n",
      "learning_rate: 1.1971513579812125e-12\n",
      "Iteration 64/1000: Loss = 4513.67867624669\n",
      "gradient in iteration64: [5136793.38278052  -13217.89150097], params: [0.08009705 0.0400152 ]\n",
      "learning_rate: 1.946739776126063e-12\n",
      "Iteration 65/1000: Loss = 4513.328222860535\n",
      "gradient in iteration65: [-8476317.8339017    -14685.95714869], params: [0.08008705 0.04001523]\n",
      "learning_rate: 1.179757554631117e-12\n",
      "Iteration 66/1000: Loss = 4513.6780179572725\n",
      "gradient in iteration66: [5137779.22324152  -13219.10226579], params: [0.08009705 0.04001525]\n",
      "learning_rate: 1.9463662344157345e-12\n",
      "Iteration 67/1000: Loss = 4513.327653702161\n",
      "gradient in iteration67: [-8597735.35717068   -14134.83309989], params: [0.08008705 0.04001527]\n",
      "learning_rate: 1.1630969766544172e-12\n",
      "Iteration 68/1000: Loss = 4513.677397508817\n",
      "gradient in iteration68: [5138744.83262953  -13220.27883361], params: [0.08009705 0.04001529]\n",
      "learning_rate: 1.9460004973399195e-12\n",
      "Iteration 69/1000: Loss = 4513.327096235627\n",
      "gradient in iteration69: [-8717464.76013868   -13592.77980555], params: [0.08008705 0.04001531]\n",
      "learning_rate: 1.1471225035202683e-12\n",
      "Iteration 70/1000: Loss = 4513.676812907111\n",
      "gradient in iteration70: [5139691.08237689  -13221.42370555], params: [0.08009705 0.04001533]\n",
      "learning_rate: 1.9456422262980506e-12\n",
      "Iteration 71/1000: Loss = 4513.326549959583\n",
      "gradient in iteration71: [-8835553.21106542   -13059.46975663], params: [0.08008705 0.04001536]\n",
      "learning_rate: 1.1317910447844127e-12\n",
      "Iteration 72/1000: Loss = 4513.6762623054565\n",
      "gradient in iteration72: [5140618.78211578  -13222.53606304], params: [0.08009705 0.04001537]\n",
      "learning_rate: 1.9452911067418608e-12\n",
      "Iteration 73/1000: Loss = 4513.3260144065\n",
      "gradient in iteration73: [-8952045.62921208   -12534.51218872], params: [0.08008705 0.0400154 ]\n",
      "learning_rate: 1.1170631176597518e-12\n",
      "Iteration 74/1000: Loss = 4513.67574399144\n",
      "gradient in iteration74: [5141528.67804114  -13223.68725596], params: [0.08009705 0.04001541]\n",
      "learning_rate: 1.944946848727853e-12\n",
      "Iteration 75/1000: Loss = 4513.325489138311\n",
      "gradient in iteration75: [-9241055.19770819   -12017.42188641], params: [0.08008705 0.04001544]\n",
      "learning_rate: 1.0821275044953774e-12\n",
      "Iteration 76/1000: Loss = 4513.675256367684\n",
      "gradient in iteration76: [5142415.80931412  -13224.71516687], params: [0.08009705 0.04001545]\n",
      "learning_rate: 1.9446113209841307e-12\n",
      "Iteration 77/1000: Loss = 4513.324977048633\n",
      "gradient in iteration77: [-10002278.75274504    -11511.29599748], params: [0.08008705 0.04001547]\n",
      "learning_rate: 9.997721766408064e-13\n",
      "Iteration 78/1000: Loss = 4513.674800829225\n",
      "gradient in iteration78: [5143268.7318993  -13225.757084 ], params: [0.08009705 0.04001549]\n",
      "learning_rate: 1.9442888406702437e-12\n",
      "Iteration 79/1000: Loss = 4513.324484730856\n",
      "gradient in iteration79: [-10730434.69059485    -11022.84913941], params: [0.08008705 0.04001551]\n",
      "learning_rate: 9.31928695187431e-13\n",
      "Iteration 80/1000: Loss = 4513.674381423285\n",
      "gradient in iteration80: [5144093.36456083  -13226.74229414], params: [0.08009705 0.04001552]\n",
      "learning_rate: 1.9439771581310977e-12\n",
      "Iteration 81/1000: Loss = 4513.324008755188\n",
      "gradient in iteration81: [-1.14308523e+07 -1.05487394e+04], params: [0.08008705 0.04001555]\n",
      "learning_rate: 8.748254061788098e-13\n",
      "Iteration 82/1000: Loss = 4513.673993290692\n",
      "gradient in iteration82: [5144894.11216346  -13227.7146804 ], params: [0.08009705 0.04001556]\n",
      "learning_rate: 1.9436745989306564e-12\n",
      "Iteration 83/1000: Loss = 4513.323546585184\n",
      "gradient in iteration83: [-1.21074308e+07 -1.00866357e+04], params: [0.08008705 0.04001558]\n",
      "learning_rate: 8.259390620102077e-13\n",
      "Iteration 84/1000: Loss = 4513.673632797852\n",
      "gradient in iteration84: [5145674.28017472  -13228.6583721 ], params: [0.08009705 0.04001559]\n",
      "learning_rate: 1.9433799062113316e-12\n",
      "Iteration 85/1000: Loss = 4513.32309628061\n",
      "gradient in iteration85: [-1.27631001e+07 -9.63481039e+03], params: [0.08008705 0.04001562]\n",
      "learning_rate: 7.835087038512337e-13\n",
      "Iteration 86/1000: Loss = 4513.673297141139\n",
      "gradient in iteration86: [5146436.57093075  -13229.59310528], params: [0.08009705 0.04001562]\n",
      "learning_rate: 1.9430920525639504e-12\n",
      "Iteration 87/1000: Loss = 4513.322656316016\n",
      "gradient in iteration87: [-1.34001061e+07 -9.19160830e+03], params: [0.08008705 0.04001565]\n",
      "learning_rate: 7.462627487957425e-13\n",
      "Iteration 88/1000: Loss = 4513.672984097363\n",
      "gradient in iteration88: [5147183.07808835  -13230.47918049], params: [0.08009705 0.04001566]\n",
      "learning_rate: 1.9428102416970914e-12\n",
      "Iteration 89/1000: Loss = 4513.322225471278\n",
      "gradient in iteration89: [-1.40201863e+07 -8.75603127e+03], params: [0.08008705 0.04001568]\n",
      "learning_rate: 7.132572820476665e-13\n",
      "Iteration 90/1000: Loss = 4513.672691871202\n",
      "gradient in iteration90: [5147915.53864598  -13231.33656115], params: [0.08009705 0.04001569]\n",
      "learning_rate: 1.9425338129440716e-12\n",
      "Iteration 91/1000: Loss = 4513.321802748944\n",
      "gradient in iteration91: [-1.46247099e+07 -8.32728206e+03], params: [0.08008705 0.04001571]\n",
      "learning_rate: 6.837742493583159e-13\n",
      "Iteration 92/1000: Loss = 4513.672418977627\n",
      "gradient in iteration92: [5148635.40156512  -13232.23650616], params: [0.08009705 0.04001572]\n",
      "learning_rate: 1.9422622151415374e-12\n",
      "Iteration 93/1000: Loss = 4513.3213873193\n",
      "gradient in iteration93: [-1.52148011e+07 -7.90432946e+03], params: [0.08008705 0.04001575]\n",
      "learning_rate: 6.572547324487559e-13\n",
      "Iteration 94/1000: Loss = 4513.672164183626\n",
      "gradient in iteration94: [5149343.83122636  -13233.09938925], params: [0.08009705 0.04001575]\n",
      "learning_rate: 1.9419950051419295e-12\n",
      "Iteration 95/1000: Loss = 4513.320978488274\n",
      "gradient in iteration95: [-1.57914863e+07 -7.48668690e+03], params: [0.08008705 0.04001578]\n",
      "learning_rate: 6.332526171925098e-13\n",
      "Iteration 96/1000: Loss = 4513.671926434635\n",
      "gradient in iteration96: [5150041.84953796  -13233.92102677], params: [0.08009705 0.04001578]\n",
      "learning_rate: 1.9417317940624433e-12\n",
      "Iteration 97/1000: Loss = 4513.320575666244\n",
      "gradient in iteration97: [-1.63571197e+07 -7.07368922e+03], params: [0.08008705 0.04001581]\n",
      "learning_rate: 6.113545782247301e-13\n",
      "Iteration 98/1000: Loss = 4513.67170483951\n",
      "gradient in iteration98: [5150730.37254297  -13234.73343291], params: [0.08009705 0.04001581]\n",
      "learning_rate: 1.941472233395689e-12\n",
      "Iteration 99/1000: Loss = 4513.320178354909\n",
      "gradient in iteration99: [-1.69134568e+07 -6.66490296e+03], params: [0.08008705 0.04001584]\n",
      "learning_rate: 5.912451902792245e-13\n",
      "Iteration 100/1000: Loss = 4513.671498623149\n",
      "gradient in iteration100: [5151410.07599595  -13235.56889474], params: [0.08009705 0.04001584]\n",
      "learning_rate: 1.9412160655966878e-12\n",
      "Iteration 101/1000: Loss = 4513.319786125031\n",
      "gradient in iteration101: [-1.74654655e+07 -6.26000201e+03], params: [0.08008705 0.04001587]\n",
      "learning_rate: 5.725584589839814e-13\n",
      "Iteration 102/1000: Loss = 4513.67130711312\n",
      "gradient in iteration102: [5152081.65666081  -13236.36902271], params: [0.08009705 0.04001587]\n",
      "learning_rate: 1.9409630255125345e-12\n",
      "Iteration 103/1000: Loss = 4513.319398611743\n",
      "gradient in iteration103: [-1.80302000e+07 -5.85854068e+03], params: [0.08008705 0.0400159 ]\n",
      "learning_rate: 5.546250180386535e-13\n",
      "Iteration 104/1000: Loss = 4513.671129720564\n",
      "gradient in iteration104: [5152745.52881237  -13237.18443018], params: [0.08009705 0.0400159 ]\n",
      "learning_rate: 1.940712954692494e-12\n",
      "Iteration 105/1000: Loss = 4513.319015531322\n",
      "gradient in iteration105: [-1.86469342e+07 -5.46037936e+03], params: [0.08008705 0.04001592]\n",
      "learning_rate: 5.362811873953383e-13\n",
      "Iteration 106/1000: Loss = 4513.670965937335\n",
      "gradient in iteration106: [5153402.05479333  -13237.95636381], params: [0.08009705 0.04001593]\n",
      "learning_rate: 1.9404657144300826e-12\n",
      "Iteration 107/1000: Loss = 4513.318636700674\n",
      "gradient in iteration107: [-1.92004498e+07 -5.06521242e+03], params: [0.08008705 0.04001595]\n",
      "learning_rate: 5.208211316242032e-13\n",
      "Iteration 108/1000: Loss = 4513.670815333703\n",
      "gradient in iteration108: [5154051.96583742  -13238.72002104], params: [0.08009705 0.04001596]\n",
      "learning_rate: 1.9402210273165554e-12\n",
      "Iteration 109/1000: Loss = 4513.3182617125885\n",
      "gradient in iteration109: [-1.95440146e+07 -4.67275531e+03], params: [0.08008705 0.04001598]\n",
      "learning_rate: 5.116656019675411e-13\n",
      "Iteration 110/1000: Loss = 4513.670677421353\n",
      "gradient in iteration110: [5154696.20258955  -13239.51573795], params: [0.08009705 0.04001598]\n",
      "learning_rate: 1.9399785374308447e-12\n",
      "Iteration 111/1000: Loss = 4513.3178899990535\n",
      "gradient in iteration111: [-1.97513440e+07 -4.28234525e+03], params: [0.08008705 0.04001601]\n",
      "learning_rate: 5.062946593638247e-13\n",
      "Iteration 112/1000: Loss = 4513.67055170886\n",
      "gradient in iteration112: [5155335.32329755  -13240.29581156], params: [0.08009705 0.04001601]\n",
      "learning_rate: 1.9397380331030763e-12\n",
      "Iteration 113/1000: Loss = 4513.317521234943\n",
      "gradient in iteration113: [-1.98748534e+07 -3.89383035e+03], params: [0.08008705 0.04001604]\n",
      "learning_rate: 5.031483661920603e-13\n",
      "Iteration 114/1000: Loss = 4513.670437848585\n",
      "gradient in iteration114: [5155969.73687025  -13241.05755884], params: [0.08009705 0.04001604]\n",
      "learning_rate: 1.9394993590614352e-12\n",
      "Iteration 115/1000: Loss = 4513.3171552377435\n",
      "gradient in iteration115: [-2.04272740e+07 -3.50683358e+03], params: [0.08008705 0.04001607]\n",
      "learning_rate: 4.895415811073203e-13\n",
      "Iteration 116/1000: Loss = 4513.670335566463\n",
      "gradient in iteration116: [5156598.50234442  -13241.79306721], params: [0.08009705 0.04001607]\n",
      "learning_rate: 1.9392628678485535e-12\n",
      "Iteration 117/1000: Loss = 4513.316792452763\n",
      "gradient in iteration117: [-2.09740894e+07 -3.12285124e+03], params: [0.08008705 0.04001609]\n",
      "learning_rate: 4.767787434253551e-13\n",
      "Iteration 118/1000: Loss = 4513.6702447539055\n",
      "gradient in iteration118: [5157222.12289802  -13242.55945292], params: [0.08009705 0.04001609]\n",
      "learning_rate: 1.939028368702618e-12\n",
      "Iteration 119/1000: Loss = 4513.316432685839\n",
      "gradient in iteration119: [-2.15156566e+07 -2.74143581e+03], params: [0.08008705 0.04001612]\n",
      "learning_rate: 4.647778210976648e-13\n",
      "Iteration 120/1000: Loss = 4513.670165090833\n",
      "gradient in iteration120: [5157840.77142598  -13243.280182  ], params: [0.08009705 0.04001612]\n",
      "learning_rate: 1.938795795209342e-12\n",
      "Iteration 121/1000: Loss = 4513.316075765427\n",
      "gradient in iteration121: [-2.20522988e+07 -2.36162173e+03], params: [0.08008705 0.04001615]\n",
      "learning_rate: 4.5346746245696746e-13\n",
      "Iteration 122/1000: Loss = 4513.670096322387\n",
      "gradient in iteration122: [5158454.78817027  -13244.00550402], params: [0.08009705 0.04001615]\n",
      "learning_rate: 1.938565018139287e-12\n",
      "Iteration 123/1000: Loss = 4513.315721538336\n",
      "gradient in iteration123: [-2.25843100e+07 -1.98337152e+03], params: [0.08008705 0.04001617]\n",
      "learning_rate: 4.4278527899625815e-13\n",
      "Iteration 124/1000: Loss = 4513.670038213883\n",
      "gradient in iteration124: [5159064.37899503  -13244.76197624], params: [0.08009705 0.04001617]\n",
      "learning_rate: 1.9383359588833e-12\n",
      "Iteration 125/1000: Loss = 4513.315369865453\n",
      "gradient in iteration125: [-2.3111963e+07 -1.6065087e+03], params: [0.08008705 0.0400162 ]\n",
      "learning_rate: 4.326763591330517e-13\n",
      "Iteration 126/1000: Loss = 4513.669990552685\n",
      "gradient in iteration126: [5159669.80401572  -13245.45860371], params: [0.08009705 0.0400162 ]\n",
      "learning_rate: 1.938108518536806e-12\n",
      "Iteration 127/1000: Loss = 4513.315020619861\n",
      "gradient in iteration127: [-2.36355068e+07 -1.23100399e+03], params: [0.08008705 0.04001623]\n",
      "learning_rate: 4.230922597353573e-13\n",
      "Iteration 128/1000: Loss = 4513.6699531405875\n",
      "gradient in iteration128: [5160271.20665967  -13246.18419858], params: [0.08009705 0.04001623]\n",
      "learning_rate: 1.937882642116627e-12\n",
      "Iteration 129/1000: Loss = 4513.314673685351\n",
      "gradient in iteration129: [-2.41551759e+07 -8.56688212e+02], params: [0.08008705 0.04001625]\n",
      "learning_rate: 4.1398994755017553e-13\n",
      "Iteration 130/1000: Loss = 4513.669925798465\n",
      "gradient in iteration130: [5160868.82084892  -13246.89228568], params: [0.08009705 0.04001625]\n",
      "learning_rate: 1.937658240721392e-12\n",
      "Iteration 131/1000: Loss = 4513.314328954292\n",
      "gradient in iteration131: [-2.46711852e+07 -4.83484200e+02], params: [0.08008705 0.04001628]\n",
      "learning_rate: 4.0533115518390637e-13\n",
      "Iteration 132/1000: Loss = 4513.669908359045\n",
      "gradient in iteration132: [5161462.81370311  -13247.63161392], params: [0.08009705 0.04001628]\n",
      "learning_rate: 1.9374352506136654e-12\n",
      "Iteration 133/1000: Loss = 4513.313986328149\n",
      "gradient in iteration133: [-2.51837358e+07 -1.11221880e+02], params: [0.08008705 0.0400163 ]\n",
      "learning_rate: 3.9708167497188406e-13\n",
      "Iteration 134/1000: Loss = 4513.669900668301\n",
      "gradient in iteration134: [5162053.28794967  -13248.34242951], params: [0.08009705 0.0400163 ]\n",
      "learning_rate: 1.9372136322854426e-12\n",
      "Iteration 135/1000: Loss = 4513.313645715168\n",
      "gradient in iteration135: [-2.60720865e+07  2.60114757e+02], params: [0.08008705 0.04001633]\n",
      "learning_rate: 3.835519648658034e-13\n",
      "Iteration 136/1000: Loss = 4513.66990257931\n",
      "gradient in iteration136: [5162640.47323602  -13249.07188973], params: [0.08009705 0.04001633]\n",
      "learning_rate: 1.9369932986504953e-12\n",
      "Iteration 137/1000: Loss = 4513.313307011322\n",
      "gradient in iteration137: [-2.65381087e+07  6.30533759e+02], params: [0.08008705 0.04001636]\n",
      "learning_rate: 3.768166041915109e-13\n",
      "Iteration 138/1000: Loss = 4513.669913960586\n",
      "gradient in iteration138: [5163224.51285305  -13249.72627117], params: [0.08009705 0.04001636]\n",
      "learning_rate: 1.9367741950996993e-12\n",
      "Iteration 139/1000: Loss = 4513.312970136093\n",
      "gradient in iteration139: [-2.70187866e+07  1.00013003e+03], params: [0.08008705 0.04001638]\n",
      "learning_rate: 3.701128452342607e-13\n",
      "Iteration 140/1000: Loss = 4513.6699346881005\n",
      "gradient in iteration140: [5163805.51039219  -13250.44800069], params: [0.08009705 0.04001638]\n",
      "learning_rate: 1.936556281965874e-12\n",
      "Iteration 141/1000: Loss = 4513.312635020117\n",
      "gradient in iteration141: [-2.74005222e+07  1.36910999e+03], params: [0.08008705 0.04001641]\n",
      "learning_rate: 3.649565479008293e-13\n",
      "Iteration 142/1000: Loss = 4513.669964645485\n",
      "gradient in iteration142: [5164383.56471554  -13251.12516497], params: [0.08009705 0.04001641]\n",
      "learning_rate: 1.936339521394713e-12\n",
      "Iteration 143/1000: Loss = 4513.312301623789\n",
      "gradient in iteration143: [-2.77204348e+07  1.73730787e+03], params: [0.08008705 0.04001643]\n",
      "learning_rate: 3.607447024890215e-13\n",
      "Iteration 144/1000: Loss = 4513.670003722697\n",
      "gradient in iteration144: [5164958.65322213  -13251.81215179], params: [0.08009705 0.04001643]\n",
      "learning_rate: 1.9361239211008126e-12\n",
      "Iteration 145/1000: Loss = 4513.311969913603\n",
      "gradient in iteration145: [-2.80090707e+07  2.10485805e+03], params: [0.08008705 0.04001646]\n",
      "learning_rate: 3.570271966650251e-13\n",
      "Iteration 146/1000: Loss = 4513.670051803451\n",
      "gradient in iteration146: [5165530.93889343  -13252.49654656], params: [0.08009705 0.04001646]\n",
      "learning_rate: 1.9359094192439838e-12\n",
      "Iteration 147/1000: Loss = 4513.311639860369\n",
      "gradient in iteration147: [-2.84564933e+07  2.47157541e+03], params: [0.08008705 0.04001648]\n",
      "learning_rate: 3.514136441556826e-13\n",
      "Iteration 148/1000: Loss = 4513.670108786562\n",
      "gradient in iteration148: [5166100.5166352   -13253.14242422], params: [0.08009705 0.04001648]\n",
      "learning_rate: 1.9356959795496267e-12\n",
      "Iteration 149/1000: Loss = 4513.311311361728\n",
      "gradient in iteration149: [-2.88999238e+07  2.83770814e+03], params: [0.08008705 0.04001651]\n",
      "learning_rate: 3.460216735862954e-13\n",
      "Iteration 150/1000: Loss = 4513.670174581365\n",
      "gradient in iteration150: [5166667.49299475  -13253.84773736], params: [0.08009705 0.04001651]\n",
      "learning_rate: 1.93548356141721e-12\n",
      "Iteration 151/1000: Loss = 4513.310984369357\n",
      "gradient in iteration151: [-2.93401334e+07  3.20318277e+03], params: [0.08008705 0.04001653]\n",
      "learning_rate: 3.40830079431092e-13\n",
      "Iteration 152/1000: Loss = 4513.670249095205\n",
      "gradient in iteration152: [5167231.93386497  -13254.50793956], params: [0.08009705 0.04001653]\n",
      "learning_rate: 1.935272139510919e-12\n",
      "Iteration 153/1000: Loss = 4513.310658835473\n",
      "gradient in iteration153: [-2.97790259e+07  3.56820456e+03], params: [0.08008705 0.04001656]\n",
      "learning_rate: 3.358068199231485e-13\n",
      "Iteration 154/1000: Loss = 4513.670332241793\n",
      "gradient in iteration154: [5167793.94142759  -13255.17855548], params: [0.08009705 0.04001655]\n",
      "learning_rate: 1.9350616749315526e-12\n",
      "Iteration 155/1000: Loss = 4513.31033471695\n",
      "gradient in iteration155: [-3.02142177e+07  3.93234254e+03], params: [0.08008705 0.04001658]\n",
      "learning_rate: 3.309700124561019e-13\n",
      "Iteration 156/1000: Loss = 4513.67042393801\n",
      "gradient in iteration156: [5168353.60554069  -13255.80988123], params: [0.08009705 0.04001658]\n",
      "learning_rate: 1.9348521334298004e-12\n",
      "Iteration 157/1000: Loss = 4513.3100119703295\n",
      "gradient in iteration157: [-3.06358443e+07  4.29618435e+03], params: [0.08008705 0.0400166 ]\n",
      "learning_rate: 3.2641502909197135e-13\n",
      "Iteration 158/1000: Loss = 4513.670524103961\n",
      "gradient in iteration158: [5168910.94653147  -13256.5052354 ], params: [0.08009705 0.0400166 ]\n",
      "learning_rate: 1.9346435068126627e-12\n",
      "Iteration 159/1000: Loss = 4513.309690565304\n",
      "gradient in iteration159: [-3.10210580e+07  4.65918833e+03], params: [0.08008705 0.04001663]\n",
      "learning_rate: 3.223616682159366e-13\n",
      "Iteration 160/1000: Loss = 4513.670632659499\n",
      "gradient in iteration160: [5169466.00714618  -13257.17598775], params: [0.08009705 0.04001663]\n",
      "learning_rate: 1.934435778507135e-12\n",
      "Iteration 161/1000: Loss = 4513.309370483037\n",
      "gradient in iteration161: [-3.14028869e+07  5.02159446e+03], params: [0.08008705 0.04001665]\n",
      "learning_rate: 3.184420601533641e-13\n",
      "Iteration 162/1000: Loss = 4513.67074951929\n",
      "gradient in iteration162: [5170018.81526085  -13257.84523942], params: [0.08009705 0.04001665]\n",
      "learning_rate: 1.9342289375199235e-12\n",
      "Iteration 163/1000: Loss = 4513.309051692376\n",
      "gradient in iteration163: [-3.17815714e+07  5.38334157e+03], params: [0.08008705 0.04001668]\n",
      "learning_rate: 3.1464775192337207e-13\n",
      "Iteration 164/1000: Loss = 4513.670874608714\n",
      "gradient in iteration164: [5170569.39393115  -13258.4795665 ], params: [0.08009705 0.04001668]\n",
      "learning_rate: 1.9340229746722486e-12\n",
      "Iteration 165/1000: Loss = 4513.308734161514\n",
      "gradient in iteration165: [-3.21573006e+07  5.74422879e+03], params: [0.08008705 0.0400167 ]\n",
      "learning_rate: 3.1097137569013745e-13\n",
      "Iteration 166/1000: Loss = 4513.671007850479\n",
      "gradient in iteration166: [5171117.95452365  -13259.14836343], params: [0.08009705 0.0400167 ]\n",
      "learning_rate: 1.9338178103734962e-12\n",
      "Iteration 167/1000: Loss = 4513.308417860423\n",
      "gradient in iteration167: [-3.25302271e+07  6.10437569e+03], params: [0.08008705 0.04001672]\n",
      "learning_rate: 3.0740640030863636e-13\n",
      "Iteration 168/1000: Loss = 4513.671149171326\n",
      "gradient in iteration168: [5171664.42746202  -13259.79733338], params: [0.08009705 0.04001672]\n",
      "learning_rate: 1.9336134701430106e-12\n",
      "Iteration 169/1000: Loss = 4513.3081027586995\n",
      "gradient in iteration169: [-3.29004700e+07  6.46387271e+03], params: [0.08008705 0.04001675]\n",
      "learning_rate: 3.0394702601011847e-13\n",
      "Iteration 170/1000: Loss = 4513.67129850151\n",
      "gradient in iteration170: [5172208.87218173  -13260.4413011 ], params: [0.08009705 0.04001675]\n",
      "learning_rate: 1.933409931254734e-12\n",
      "Iteration 171/1000: Loss = 4513.307788830683\n",
      "gradient in iteration171: [-3.32681248e+07  6.82239420e+03], params: [0.08008705 0.04001677]\n",
      "learning_rate: 3.0058802757948767e-13\n",
      "Iteration 172/1000: Loss = 4513.671455767913\n",
      "gradient in iteration172: [5172751.24529988  -13261.10213995], params: [0.08009705 0.04001677]\n",
      "learning_rate: 1.933207209429665e-12\n",
      "Iteration 173/1000: Loss = 4513.307476046969\n",
      "gradient in iteration173: [-3.36332712e+07  7.17998655e+03], params: [0.08008705 0.0400168 ]\n",
      "learning_rate: 2.9732463232259354e-13\n",
      "Iteration 174/1000: Loss = 4513.671620892421\n",
      "gradient in iteration174: [5173291.81366222  -13261.73951384], params: [0.08009705 0.04001679]\n",
      "learning_rate: 1.9330052044601204e-12\n",
      "Iteration 175/1000: Loss = 4513.30716438209\n",
      "gradient in iteration175: [-3.39959695e+07  7.53653326e+03], params: [0.08008705 0.04001682]\n",
      "learning_rate: 2.941525173594171e-13\n",
      "Iteration 176/1000: Loss = 4513.671793815682\n",
      "gradient in iteration176: [5173830.42652     -13262.35860688], params: [0.08009705 0.04001682]\n",
      "learning_rate: 1.9328039722256922e-12\n",
      "Iteration 177/1000: Loss = 4513.306853810167\n",
      "gradient in iteration177: [-3.43562704e+07  7.89184710e+03], params: [0.08008705 0.04001684]\n",
      "learning_rate: 2.9106768218115835e-13\n",
      "Iteration 178/1000: Loss = 4513.671974452549\n",
      "gradient in iteration178: [5174367.24117033  -13263.00839537], params: [0.08009705 0.04001684]\n",
      "learning_rate: 1.9326034535071416e-12\n",
      "Iteration 179/1000: Loss = 4513.306544305557\n",
      "gradient in iteration179: [-3.47142161e+07  8.24582989e+03], params: [0.08008705 0.04001687]\n",
      "learning_rate: 2.880664213359329e-13\n",
      "Iteration 180/1000: Loss = 4513.672162740361\n",
      "gradient in iteration180: [5174902.25179245  -13263.65118075], params: [0.08009705 0.04001686]\n",
      "learning_rate: 1.932403650046969e-12\n",
      "Iteration 181/1000: Loss = 4513.306235843459\n",
      "gradient in iteration181: [-3.50698398e+07  8.59861625e+03], params: [0.08008705 0.04001689]\n",
      "learning_rate: 2.8514530015243576e-13\n",
      "Iteration 182/1000: Loss = 4513.672358602087\n",
      "gradient in iteration182: [5175435.51027302  -13264.25986008], params: [0.08009705 0.04001689]\n",
      "learning_rate: 1.9322045420429696e-12\n",
      "Iteration 183/1000: Loss = 4513.305928400011\n",
      "gradient in iteration183: [-3.54231670e+07  8.94979912e+03], params: [0.08008705 0.04001691]\n",
      "learning_rate: 2.8230112810852067e-13\n",
      "Iteration 184/1000: Loss = 4513.672561967065\n",
      "gradient in iteration184: [5175967.02261471  -13264.8919589 ], params: [0.08009705 0.04001691]\n",
      "learning_rate: 1.9320061268374074e-12\n",
      "Iteration 185/1000: Loss = 4513.305621951608\n",
      "gradient in iteration185: [-3.57742200e+07  1.05947781e+04], params: [0.08008705 0.04001694]\n",
      "learning_rate: 2.7953090265969307e-13\n",
      "Iteration 186/1000: Loss = 4513.672772753984\n",
      "gradient in iteration186: [5176488.52774073  -13265.5222842 ], params: [0.08009705 0.04001693]\n",
      "learning_rate: 1.9318114869588022e-12\n",
      "Iteration 187/1000: Loss = 4513.30532127915\n",
      "gradient in iteration187: [-3.61175414e+07  1.22889708e+04], params: [0.08008705 0.04001696]\n",
      "learning_rate: 2.76873774318001e-13\n",
      "Iteration 188/1000: Loss = 4513.673046657632\n",
      "gradient in iteration188: [5176999.89436551  -13266.1367843 ], params: [0.08009705 0.04001695]\n",
      "learning_rate: 1.931620669122227e-12\n",
      "Iteration 189/1000: Loss = 4513.3050264591275\n",
      "gradient in iteration189: [-3.64530838e+07  1.26763824e+04], params: [0.08008705 0.04001698]\n",
      "learning_rate: 2.743252134784421e-13\n",
      "Iteration 190/1000: Loss = 4513.673324052676\n",
      "gradient in iteration190: [5177509.58752003  -13266.74450866], params: [0.08009705 0.04001698]\n",
      "learning_rate: 1.931430513253744e-12\n",
      "Iteration 191/1000: Loss = 4513.304732638409\n",
      "gradient in iteration191: [-3.69084198e+07  1.30644934e+04], params: [0.08008705 0.040017  ]\n",
      "learning_rate: 2.709408872841564e-13\n",
      "Iteration 192/1000: Loss = 4513.673609082214\n",
      "gradient in iteration192: [5178017.82666538  -13267.29211542], params: [0.08009705 0.040017  ]\n",
      "learning_rate: 1.9312409371212905e-12\n",
      "Iteration 193/1000: Loss = 4513.304439647734\n",
      "gradient in iteration193: [-3.76338358e+07  1.34529018e+04], params: [0.08008705 0.04001702]\n",
      "learning_rate: 2.657183303065966e-13\n",
      "Iteration 194/1000: Loss = 4513.6739018649105\n",
      "gradient in iteration194: [5178525.20970337  -13267.91261818], params: [0.08009705 0.04001702]\n",
      "learning_rate: 1.9310517174392236e-12\n",
      "Iteration 195/1000: Loss = 4513.304147128357\n",
      "gradient in iteration195: [-3.82413286e+07  1.38423077e+04], params: [0.08008705 0.04001705]\n",
      "learning_rate: 2.614971906506511e-13\n",
      "Iteration 196/1000: Loss = 4513.674202742021\n",
      "gradient in iteration196: [5179031.56946886  -13268.577095  ], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.930862916331973e-12\n",
      "Iteration 197/1000: Loss = 4513.3038552097\n",
      "gradient in iteration197: [-3.8775548e+07  1.4232686e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.5789448571349823e-13\n",
      "Iteration 198/1000: Loss = 4513.6745115649555\n",
      "gradient in iteration198: [5179536.79172933  -13269.13657067], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930674576145104e-12\n",
      "Iteration 199/1000: Loss = 4513.303563968655\n",
      "gradient in iteration199: [-39259143.93616949    176026.57803764], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5471772935901923e-13\n",
      "Iteration 200/1000: Loss = 4513.674828228438\n",
      "gradient in iteration200: [5179094.43542521  -13268.64166913], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9308394787319574e-12\n",
      "Iteration 201/1000: Loss = 4513.303818978877\n",
      "gradient in iteration201: [-3.88381535e+07  1.42812578e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.574787701105747e-13\n",
      "Iteration 202/1000: Loss = 4513.674550491656\n",
      "gradient in iteration202: [5179599.49247597  -13269.21137661], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930651204697636e-12\n",
      "Iteration 203/1000: Loss = 4513.3035278262205\n",
      "gradient in iteration203: [-39316362.07435076    221350.83631838], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5434703193263673e-13\n",
      "Iteration 204/1000: Loss = 4513.675675053043\n",
      "gradient in iteration204: [5178893.3208627   -13268.39092144], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.930914459990113e-12\n",
      "Iteration 205/1000: Loss = 4513.303934935215\n",
      "gradient in iteration205: [-3.86351795e+07  1.41259951e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.5883146233772533e-13\n",
      "Iteration 206/1000: Loss = 4513.674426371513\n",
      "gradient in iteration206: [5179398.82524843  -13268.98050138], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307260045803397e-12\n",
      "Iteration 207/1000: Loss = 4513.303643503821\n",
      "gradient in iteration207: [-39131401.69847793     76214.13092238], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.555492409153584e-13\n",
      "Iteration 208/1000: Loss = 4513.674740900289\n",
      "gradient in iteration208: [5179540.20197067  -13269.13879893], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306733049770106e-12\n",
      "Iteration 209/1000: Loss = 4513.303562000885\n",
      "gradient in iteration209: [-39262271.81309514    178494.57458397], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.54697436959435e-13\n",
      "Iteration 210/1000: Loss = 4513.674830397329\n",
      "gradient in iteration210: [5179083.46655531  -13268.59278379], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.93084356809008e-12\n",
      "Iteration 211/1000: Loss = 4513.303825305233\n",
      "gradient in iteration211: [-3.88272712e+07  1.42727701e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.575509350343457e-13\n",
      "Iteration 212/1000: Loss = 4513.674543682016\n",
      "gradient in iteration212: [5179588.5399315   -13269.20482825], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306552871731097e-12\n",
      "Iteration 213/1000: Loss = 4513.303534137865\n",
      "gradient in iteration213: [-39306406.44266532    213437.69171835], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5441145362872594e-13\n",
      "Iteration 214/1000: Loss = 4513.675509720361\n",
      "gradient in iteration214: [5178928.32140205  -13268.4025175 ], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.93090141036993e-12\n",
      "Iteration 215/1000: Loss = 4513.303914723246\n",
      "gradient in iteration215: [-3.86711168e+07  1.41529077e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.5859092851367244e-13\n",
      "Iteration 216/1000: Loss = 4513.674447908014\n",
      "gradient in iteration216: [5179433.80068573  -13269.002784  ], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307129668644582e-12\n",
      "Iteration 217/1000: Loss = 4513.303623339913\n",
      "gradient in iteration217: [-39164032.6081415     101527.22293242], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.553363209569277e-13\n",
      "Iteration 218/1000: Loss = 4513.674762978661\n",
      "gradient in iteration218: [5179426.7929381   -13268.96986029], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307155791128312e-12\n",
      "Iteration 219/1000: Loss = 4513.3036273865155\n",
      "gradient in iteration219: [-39157496.90413577     96447.02463447], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.553789386610106e-13\n",
      "Iteration 220/1000: Loss = 4513.674758543113\n",
      "gradient in iteration220: [5179449.49274352  -13269.00501226], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307071174282414e-12\n",
      "Iteration 221/1000: Loss = 4513.303614275936\n",
      "gradient in iteration221: [-39178645.68575169    112904.02394479], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.552410841408118e-13\n",
      "Iteration 222/1000: Loss = 4513.67477291501\n",
      "gradient in iteration222: [5179375.8875192   -13268.95217062], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930734555122194e-12\n",
      "Iteration 223/1000: Loss = 4513.303656725501\n",
      "gradient in iteration223: [-39109909.35323752     59611.13529338], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5568967469806217e-13\n",
      "Iteration 224/1000: Loss = 4513.67472644274\n",
      "gradient in iteration224: [5179614.73983632  -13269.21214968], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930645521391809e-12\n",
      "Iteration 225/1000: Loss = 4513.303519034318\n",
      "gradient in iteration225: [-39330204.78175422    232372.67482727], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5425751163744585e-13\n",
      "Iteration 226/1000: Loss = 4513.675905342066\n",
      "gradient in iteration226: [5178844.4713111   -13268.31406914], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.930932673378462e-12\n",
      "Iteration 227/1000: Loss = 4513.3039630646335\n",
      "gradient in iteration227: [-3.85847419e+07  1.40882109e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.591698043860567e-13\n",
      "Iteration 228/1000: Loss = 4513.674396464314\n",
      "gradient in iteration228: [5179350.14604518  -13268.91897406], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307441509116244e-12\n",
      "Iteration 229/1000: Loss = 4513.303671567012\n",
      "gradient in iteration229: [-39085694.48581147     40971.91517758], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5584808282299063e-13\n",
      "Iteration 230/1000: Loss = 4513.674710240184\n",
      "gradient in iteration230: [5179698.55927731  -13269.34138888], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306142791049274e-12\n",
      "Iteration 231/1000: Loss = 4513.303470721642\n",
      "gradient in iteration231: [-39405772.32438045    292920.07793629], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.5376992785935006e-13\n",
      "Iteration 232/1000: Loss = 4513.677170570444\n",
      "gradient in iteration232: [5178577.28332274  -13267.9779199 ], params: [0.08009705 0.04001702]\n",
      "learning_rate: 1.9310322995862843e-12\n",
      "Iteration 233/1000: Loss = 4513.304117121863\n",
      "gradient in iteration233: [-3.82990371e+07  1.38823146e+04], params: [0.08008705 0.04001705]\n",
      "learning_rate: 2.6110317022445856e-13\n",
      "Iteration 234/1000: Loss = 4513.674234088745\n",
      "gradient in iteration234: [5179083.49829668  -13268.58814537], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9308435562564015e-12\n",
      "Iteration 235/1000: Loss = 4513.303825270133\n",
      "gradient in iteration235: [-3.88273316e+07  1.42727732e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.575505344781769e-13\n",
      "Iteration 236/1000: Loss = 4513.6745437217805\n",
      "gradient in iteration236: [5179588.6085074  -13269.1865474], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306552616119245e-12\n",
      "Iteration 237/1000: Loss = 4513.303534102676\n",
      "gradient in iteration237: [-39306460.75309596    213480.73560148], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5441110210392965e-13\n",
      "Iteration 238/1000: Loss = 4513.675510619291\n",
      "gradient in iteration238: [5178928.16633321  -13268.41793343], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.930901468185494e-12\n",
      "Iteration 239/1000: Loss = 4513.303914833563\n",
      "gradient in iteration239: [-3.86709229e+07  1.41528929e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.5859222505687294e-13\n",
      "Iteration 240/1000: Loss = 4513.674447791798\n",
      "gradient in iteration240: [5179433.60714526  -13268.97881881], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930713039009625e-12\n",
      "Iteration 241/1000: Loss = 4513.303623449813\n",
      "gradient in iteration241: [-39163854.45804735    101388.97737306], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5533748244090947e-13\n",
      "Iteration 242/1000: Loss = 4513.674762857669\n",
      "gradient in iteration242: [5179427.3851101   -13269.00955974], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307153583711116e-12\n",
      "Iteration 243/1000: Loss = 4513.30362702989\n",
      "gradient in iteration243: [-39158074.54613969     96896.02452454], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5537517142772355e-13\n",
      "Iteration 244/1000: Loss = 4513.674758935094\n",
      "gradient in iteration244: [5179447.51518371  -13269.03798144], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307078545896435e-12\n",
      "Iteration 245/1000: Loss = 4513.303615434785\n",
      "gradient in iteration245: [-39176779.88288233    111450.42512908], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5525324005430425e-13\n",
      "Iteration 246/1000: Loss = 4513.674771646433\n",
      "gradient in iteration246: [5179382.38367606  -13268.94453086], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307321335295038e-12\n",
      "Iteration 247/1000: Loss = 4513.3036529789\n",
      "gradient in iteration247: [-39116006.69462132     64315.94915739], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5564981819514465e-13\n",
      "Iteration 248/1000: Loss = 4513.674730538462\n",
      "gradient in iteration248: [5179593.61204703  -13269.19018538], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306533965794854e-12\n",
      "Iteration 249/1000: Loss = 4513.303531215846\n",
      "gradient in iteration249: [-39311015.5008344     217099.88345719], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.543816249109043e-13\n",
      "Iteration 250/1000: Loss = 4513.6755862279415\n",
      "gradient in iteration250: [5178912.13203256  -13268.40570073], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9309074464013556e-12\n",
      "Iteration 251/1000: Loss = 4513.303924078649\n",
      "gradient in iteration251: [-3.86545130e+07  1.41404542e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.5870200463674895e-13\n",
      "Iteration 252/1000: Loss = 4513.674437933133\n",
      "gradient in iteration252: [5179417.61700058  -13268.96990577], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930718999598846e-12\n",
      "Iteration 253/1000: Loss = 4513.30363267358\n",
      "gradient in iteration253: [-39148948.74419588     89810.40318758], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.554347005673447e-13\n",
      "Iteration 254/1000: Loss = 4513.674752753305\n",
      "gradient in iteration254: [5179479.23649444  -13269.0585815 ], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930696030122164e-12\n",
      "Iteration 255/1000: Loss = 4513.30359713952\n",
      "gradient in iteration255: [-39206181.44243235    134410.69977489], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5506182015413335e-13\n",
      "Iteration 256/1000: Loss = 4513.674791728126\n",
      "gradient in iteration256: [5179279.79753949  -13268.82979811], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9307703755936637e-12\n",
      "Iteration 257/1000: Loss = 4513.303712108274\n",
      "gradient in iteration257: [-3.90189001e+07  1.44245997e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5628605568377186e-13\n",
      "Iteration 258/1000: Loss = 4513.674666085716\n",
      "gradient in iteration258: [5179784.40680054  -13269.42929154], params: [0.08009705 0.04001708]\n",
      "learning_rate: 1.9305822819326235e-12\n",
      "Iteration 259/1000: Loss = 4513.303421219848\n",
      "gradient in iteration259: [-39482343.54070337    347312.76982711], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.5327777186505514e-13\n",
      "Iteration 260/1000: Loss = 4513.678466409249\n",
      "gradient in iteration260: [5178349.34780694  -13267.71698587], params: [0.08009705 0.04001701]\n",
      "learning_rate: 1.931117297876987e-12\n",
      "Iteration 261/1000: Loss = 4513.3042485000315\n",
      "gradient in iteration261: [-3.80406605e+07  1.37072435e+04], params: [0.08008705 0.04001704]\n",
      "learning_rate: 2.628766130608608e-13\n",
      "Iteration 262/1000: Loss = 4513.674097502165\n",
      "gradient in iteration262: [5178856.09547193  -13268.27250523], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9309283393186733e-12\n",
      "Iteration 263/1000: Loss = 4513.30395636206\n",
      "gradient in iteration263: [-3.85968024e+07  1.40972380e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.590888199673602e-13\n",
      "Iteration 264/1000: Loss = 4513.674403582127\n",
      "gradient in iteration264: [5179361.7214571   -13268.91406279], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307398358704944e-12\n",
      "Iteration 265/1000: Loss = 4513.303664881779\n",
      "gradient in iteration265: [-39096613.44609958     49367.87518091], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5577662919031257e-13\n",
      "Iteration 266/1000: Loss = 4513.674717533199\n",
      "gradient in iteration266: [5179660.7880533   -13269.29696006], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930628357568249e-12\n",
      "Iteration 267/1000: Loss = 4513.303492494258\n",
      "gradient in iteration267: [-39371822.69523873    265638.91578917], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.5398874919776854e-13\n",
      "Iteration 268/1000: Loss = 4513.676600466069\n",
      "gradient in iteration268: [5178697.48773592  -13268.12275693], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.9309874777744387e-12\n",
      "Iteration 269/1000: Loss = 4513.304047805496\n",
      "gradient in iteration269: [-3.84296765e+07  1.39751128e+04], params: [0.08008705 0.04001705]\n",
      "learning_rate: 2.6021556512374904e-13\n",
      "Iteration 270/1000: Loss = 4513.674306853327\n",
      "gradient in iteration270: [5179203.46078426  -13268.7191126 ], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9307988333954627e-12\n",
      "Iteration 271/1000: Loss = 4513.303756111248\n",
      "gradient in iteration271: [-3.89451582e+07  1.43656923e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.567713283880187e-13\n",
      "Iteration 272/1000: Loss = 4513.674618349448\n",
      "gradient in iteration272: [5179708.27654608  -13269.3463911 ], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930610657221834e-12\n",
      "Iteration 273/1000: Loss = 4513.303465114888\n",
      "gradient in iteration273: [-39414487.71993237    299944.73156876], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.5371381384066254e-13\n",
      "Iteration 274/1000: Loss = 4513.6773173721995\n",
      "gradient in iteration274: [5178546.35340882  -13267.96896137], params: [0.08009705 0.04001702]\n",
      "learning_rate: 1.931043833066672e-12\n",
      "Iteration 275/1000: Loss = 4513.30413494453\n",
      "gradient in iteration275: [-3.82648485e+07  1.38585976e+04], params: [0.08008705 0.04001705]\n",
      "learning_rate: 2.6133645881040985e-13\n",
      "Iteration 276/1000: Loss = 4513.674215460274\n",
      "gradient in iteration276: [5179052.67192873  -13268.56831838], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9308550488782543e-12\n",
      "Iteration 277/1000: Loss = 4513.303843052431\n",
      "gradient in iteration277: [-3.87966326e+07  1.42489077e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.577543287884767e-13\n",
      "Iteration 278/1000: Loss = 4513.674524611864\n",
      "gradient in iteration278: [5179557.81388082  -13269.17772531], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306667401608615e-12\n",
      "Iteration 279/1000: Loss = 4513.303551841444\n",
      "gradient in iteration279: [-39278398.479928      191237.01913486], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.545928649588447e-13\n",
      "Iteration 280/1000: Loss = 4513.675045907687\n",
      "gradient in iteration280: [5179026.84814544  -13268.52211605], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9308646765522957e-12\n",
      "Iteration 281/1000: Loss = 4513.3038579432905\n",
      "gradient in iteration281: [-3.87707924e+07  1.42291221e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.579261187648579e-13\n",
      "Iteration 282/1000: Loss = 4513.67450863047\n",
      "gradient in iteration282: [5179532.0340716   -13269.13070443], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930676349565708e-12\n",
      "Iteration 283/1000: Loss = 4513.303566697332\n",
      "gradient in iteration283: [-39254803.93149883    172604.07366848], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5474589090931123e-13\n",
      "Iteration 284/1000: Loss = 4513.674825221038\n",
      "gradient in iteration284: [5179109.65727424  -13268.63084614], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.930833803828551e-12\n",
      "Iteration 285/1000: Loss = 4513.303810204626\n",
      "gradient in iteration285: [-3.88532085e+07  1.42930162e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.573790011774084e-13\n",
      "Iteration 286/1000: Loss = 4513.674559938432\n",
      "gradient in iteration286: [5179614.61887353  -13269.200781  ], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306455664793873e-12\n",
      "Iteration 287/1000: Loss = 4513.303519073654\n",
      "gradient in iteration287: [-39330141.36505335    232322.33565977], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.542579216073061e-13\n",
      "Iteration 288/1000: Loss = 4513.675904280066\n",
      "gradient in iteration288: [5178844.70796162  -13268.31024926], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.930932585143296e-12\n",
      "Iteration 289/1000: Loss = 4513.30396293628\n",
      "gradient in iteration289: [-3.85849732e+07  1.40884582e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.591682506657481e-13\n",
      "Iteration 290/1000: Loss = 4513.67439659984\n",
      "gradient in iteration290: [5179350.34222319  -13268.93820988], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.93074407778092e-12\n",
      "Iteration 291/1000: Loss = 4513.303671439443\n",
      "gradient in iteration291: [-39085903.87372897     41132.94526178], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5584671221384644e-13\n",
      "Iteration 292/1000: Loss = 4513.67471037863\n",
      "gradient in iteration292: [5179697.84009438  -13269.33415839], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306145471639703e-12\n",
      "Iteration 293/1000: Loss = 4513.303471139635\n",
      "gradient in iteration293: [-39405122.22907273    292396.74659038], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.537741144886513e-13\n",
      "Iteration 294/1000: Loss = 4513.677159633479\n",
      "gradient in iteration294: [5178579.57543129  -13267.99888375], params: [0.08009705 0.04001702]\n",
      "learning_rate: 1.9310314448855747e-12\n",
      "Iteration 295/1000: Loss = 4513.30411579422\n",
      "gradient in iteration295: [-3.83015746e+07  1.38841159e+04], params: [0.08008705 0.04001705]\n",
      "learning_rate: 2.610858721029284e-13\n",
      "Iteration 296/1000: Loss = 4513.67423548055\n",
      "gradient in iteration296: [5179085.81341544  -13268.58227912], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.930842693144202e-12\n",
      "Iteration 297/1000: Loss = 4513.303823944943\n",
      "gradient in iteration297: [-3.88296121e+07  1.42745851e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.5753540802624234e-13\n",
      "Iteration 298/1000: Loss = 4513.674545145853\n",
      "gradient in iteration298: [5179590.90302611  -13269.21087639], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930654406346576e-12\n",
      "Iteration 299/1000: Loss = 4513.303532780441\n",
      "gradient in iteration299: [-39308547.20692877    215138.91811082], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5439759824645307e-13\n",
      "Iteration 300/1000: Loss = 4513.675545255863\n",
      "gradient in iteration300: [5178920.76768476  -13268.40042566], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.930904226687081e-12\n",
      "Iteration 301/1000: Loss = 4513.303919069596\n",
      "gradient in iteration301: [-3.86634095e+07  1.41472335e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.586424771700207e-13\n",
      "Iteration 302/1000: Loss = 4513.674443271683\n",
      "gradient in iteration302: [5179426.27661796  -13269.00564891], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307157715795808e-12\n",
      "Iteration 303/1000: Loss = 4513.303627676649\n",
      "gradient in iteration303: [-39157029.76123612     96083.66422071], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5538198532871355e-13\n",
      "Iteration 304/1000: Loss = 4513.674758226448\n",
      "gradient in iteration304: [5179451.1733533   -13269.02374785], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307064909593044e-12\n",
      "Iteration 305/1000: Loss = 4513.303613337269\n",
      "gradient in iteration305: [-39180157.08409257    114081.89748181], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.552312380610662e-13\n",
      "Iteration 306/1000: Loss = 4513.674773946032\n",
      "gradient in iteration306: [5179370.62018063  -13268.90587734], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930736518648909e-12\n",
      "Iteration 307/1000: Loss = 4513.303659761493\n",
      "gradient in iteration307: [-39104962.80259395     55797.72710007], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5572201795667407e-13\n",
      "Iteration 308/1000: Loss = 4513.674723126655\n",
      "gradient in iteration308: [5179631.87717214  -13269.23752458], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306391336558795e-12\n",
      "Iteration 309/1000: Loss = 4513.30350915661\n",
      "gradient in iteration309: [-39345723.9266354     244754.59299538], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5415722477609367e-13\n",
      "Iteration 310/1000: Loss = 4513.676164060777\n",
      "gradient in iteration310: [5178789.71704704  -13268.22398369], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.9309530887270765e-12\n",
      "Iteration 311/1000: Loss = 4513.303994633826\n",
      "gradient in iteration311: [-3.85275369e+07  1.40459826e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.595546145797393e-13\n",
      "Iteration 312/1000: Loss = 4513.674362995947\n",
      "gradient in iteration312: [5179295.50528606  -13268.8654503 ], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9307645199610366e-12\n",
      "Iteration 313/1000: Loss = 4513.303703062612\n",
      "gradient in iteration313: [-3.90339471e+07  1.44367849e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5618726107132446e-13\n",
      "Iteration 314/1000: Loss = 4513.674675922558\n",
      "gradient in iteration314: [5179800.10799874  -13269.42942796], params: [0.08009705 0.04001708]\n",
      "learning_rate: 1.9305764298814953e-12\n",
      "Iteration 315/1000: Loss = 4513.303412195931\n",
      "gradient in iteration315: [-39496210.80614634    347287.4385352 ], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.5318884510419457e-13\n",
      "Iteration 316/1000: Loss = 4513.678702560909\n",
      "gradient in iteration316: [5178365.85254372  -13267.73153778], params: [0.08009705 0.04001701]\n",
      "learning_rate: 1.9311111429270287e-12\n",
      "Iteration 317/1000: Loss = 4513.304238984226\n",
      "gradient in iteration317: [-3.80598964e+07  1.37198026e+04], params: [0.08008705 0.04001704]\n",
      "learning_rate: 2.627437522418223e-13\n",
      "Iteration 318/1000: Loss = 4513.674107338872\n",
      "gradient in iteration318: [5178872.60766656  -13268.36500084], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.930922182792538e-12\n",
      "Iteration 319/1000: Loss = 4513.303946865713\n",
      "gradient in iteration319: [-3.86138482e+07  1.41099184e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.5897444758055336e-13\n",
      "Iteration 320/1000: Loss = 4513.6744136781\n",
      "gradient in iteration320: [5179378.16816811  -13268.93757323], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.93073370495688e-12\n",
      "Iteration 321/1000: Loss = 4513.303655405409\n",
      "gradient in iteration321: [-39112057.45253028     61268.52904345], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5567563179556204e-13\n",
      "Iteration 322/1000: Loss = 4513.674727885667\n",
      "gradient in iteration322: [5179607.29243896  -13269.19150415], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930648297332833e-12\n",
      "Iteration 323/1000: Loss = 4513.303523326318\n",
      "gradient in iteration323: [-39323449.42404597    226990.57749378], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.543011904211303e-13\n",
      "Iteration 324/1000: Loss = 4513.675792889206\n",
      "gradient in iteration324: [5178868.32699332  -13268.35604232], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.930923778825955e-12\n",
      "Iteration 325/1000: Loss = 4513.303949331597\n",
      "gradient in iteration325: [-3.86094277e+07  1.41066271e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.590040981606657e-13\n",
      "Iteration 326/1000: Loss = 4513.674411054997\n",
      "gradient in iteration326: [5179373.89831786  -13268.96540377], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930735296644207e-12\n",
      "Iteration 327/1000: Loss = 4513.303657866072\n",
      "gradient in iteration327: [-39108052.63294605     58180.3348723 ], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5570181399356194e-13\n",
      "Iteration 328/1000: Loss = 4513.674725199341\n",
      "gradient in iteration328: [5179621.12307941  -13269.2175157 ], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930643142109736e-12\n",
      "Iteration 329/1000: Loss = 4513.303515328949\n",
      "gradient in iteration329: [-39336030.60236237    237017.0609538 ], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.542198551014814e-13\n",
      "Iteration 330/1000: Loss = 4513.676002395607\n",
      "gradient in iteration330: [5178823.9274174   -13268.27177763], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.930940333201645e-12\n",
      "Iteration 331/1000: Loss = 4513.303974909589\n",
      "gradient in iteration331: [-3.85633537e+07  1.40725021e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.5931354635427684e-13\n",
      "Iteration 332/1000: Loss = 4513.6743838954235\n",
      "gradient in iteration332: [5179329.62779923  -13268.8823669 ], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307517996782016e-12\n",
      "Iteration 333/1000: Loss = 4513.3036833850365\n",
      "gradient in iteration333: [-3.90663417e+07  2.61265188e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.559748256997599e-13\n",
      "Iteration 334/1000: Loss = 4513.67469735163\n",
      "gradient in iteration334: [5179765.39299525  -13269.39031969], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9305893686851717e-12\n",
      "Iteration 335/1000: Loss = 4513.303432184421\n",
      "gradient in iteration335: [-39465454.48088727    341188.43627766], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.533861609231664e-13\n",
      "Iteration 336/1000: Loss = 4513.678179407958\n",
      "gradient in iteration336: [5178365.19138655  -13267.73622168], params: [0.08009705 0.04001701]\n",
      "learning_rate: 1.9311113894851485e-12\n",
      "Iteration 337/1000: Loss = 4513.30423937389\n",
      "gradient in iteration337: [-3.80591100e+07  1.37192621e+04], params: [0.08008705 0.04001704]\n",
      "learning_rate: 2.627491816753409e-13\n",
      "Iteration 338/1000: Loss = 4513.674106936042\n",
      "gradient in iteration338: [5178871.91526825  -13268.33821622], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9309224409505478e-12\n",
      "Iteration 339/1000: Loss = 4513.303947254583\n",
      "gradient in iteration339: [-3.86131496e+07  1.41094522e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.5897913289842877e-13\n",
      "Iteration 340/1000: Loss = 4513.674413263089\n",
      "gradient in iteration340: [5179377.49623343  -13268.95753664], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307339554361987e-12\n",
      "Iteration 341/1000: Loss = 4513.303655794434\n",
      "gradient in iteration341: [-39111425.62092209     60780.71451157], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.556797621473211e-13\n",
      "Iteration 342/1000: Loss = 4513.674727460349\n",
      "gradient in iteration342: [5179609.48500331  -13269.21760665], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306474800761176e-12\n",
      "Iteration 343/1000: Loss = 4513.30352206319\n",
      "gradient in iteration343: [-39325439.2091876     228575.59661134], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5428832331168727e-13\n",
      "Iteration 344/1000: Loss = 4513.675825991957\n",
      "gradient in iteration344: [5178861.31224259  -13268.36945736], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9309263942559077e-12\n",
      "Iteration 345/1000: Loss = 4513.303953376975\n",
      "gradient in iteration345: [-3.86021696e+07  1.41012887e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.590527968286739e-13\n",
      "Iteration 346/1000: Loss = 4513.674406757004\n",
      "gradient in iteration346: [5179366.90412171  -13268.93379883], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930737903901356e-12\n",
      "Iteration 347/1000: Loss = 4513.303661901001\n",
      "gradient in iteration347: [-39101475.37712874     53111.4075784 ], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5574482557374824e-13\n",
      "Iteration 348/1000: Loss = 4513.674720791638\n",
      "gradient in iteration348: [5179643.96408388  -13269.28513663], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930634628430237e-12\n",
      "Iteration 349/1000: Loss = 4513.303502196216\n",
      "gradient in iteration349: [-39356639.05267575    253479.65711262], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.540867370970319e-13\n",
      "Iteration 350/1000: Loss = 4513.676346369935\n",
      "gradient in iteration350: [5178751.16543109  -13268.20261056], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.930967463111847e-12\n",
      "Iteration 351/1000: Loss = 4513.304016859289\n",
      "gradient in iteration351: [-3.84868691e+07  1.40163480e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.5982887756249267e-13\n",
      "Iteration 352/1000: Loss = 4513.674339494263\n",
      "gradient in iteration352: [5179257.04643858  -13268.80906163], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9307788569552304e-12\n",
      "Iteration 353/1000: Loss = 4513.303725236459\n",
      "gradient in iteration353: [-3.89969971e+07  1.44069809e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.564300006914504e-13\n",
      "Iteration 354/1000: Loss = 4513.674651826549\n",
      "gradient in iteration354: [5179761.71486225  -13269.39209321], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9305907395907953e-12\n",
      "Iteration 355/1000: Loss = 4513.303434314474\n",
      "gradient in iteration355: [-39462168.58362277    338521.37862596], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.5340725963423384e-13\n",
      "Iteration 356/1000: Loss = 4513.678123660947\n",
      "gradient in iteration356: [5178376.86898019  -13267.76905444], params: [0.08009705 0.04001701]\n",
      "learning_rate: 1.9311070346970235e-12\n",
      "Iteration 357/1000: Loss = 4513.304232631753\n",
      "gradient in iteration357: [-3.80726893e+07  1.37284843e+04], params: [0.08008705 0.04001704]\n",
      "learning_rate: 2.626554672665368e-13\n",
      "Iteration 358/1000: Loss = 4513.674113912045\n",
      "gradient in iteration358: [5178883.57981064  -13268.33962594], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.93091809188065e-12\n",
      "Iteration 359/1000: Loss = 4513.303940527191\n",
      "gradient in iteration359: [-3.86251898e+07  1.41184201e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.5889840441766173e-13\n",
      "Iteration 360/1000: Loss = 4513.674420419436\n",
      "gradient in iteration360: [5179389.12294085  -13268.97022409], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307296213191295e-12\n",
      "Iteration 361/1000: Loss = 4513.303649083275\n",
      "gradient in iteration361: [-39122341.10234749     69208.32070136], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.556084252176811e-13\n",
      "Iteration 362/1000: Loss = 4513.674734798576\n",
      "gradient in iteration362: [5179571.63679057  -13269.19004896], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930661587720857e-12\n",
      "Iteration 363/1000: Loss = 4513.303543879028\n",
      "gradient in iteration363: [-39291010.57528897    201223.37564317], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.545111427164267e-13\n",
      "Iteration 364/1000: Loss = 4513.675254528861\n",
      "gradient in iteration364: [5178982.49545396  -13268.47891505], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9308812124346538e-12\n",
      "Iteration 365/1000: Loss = 4513.303883498052\n",
      "gradient in iteration365: [-3.87261617e+07  1.41947432e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.5822337055030126e-13\n",
      "Iteration 366/1000: Loss = 4513.674481261278\n",
      "gradient in iteration366: [5179487.83308384  -13269.10528405], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9306928256738567e-12\n",
      "Iteration 367/1000: Loss = 4513.303592189292\n",
      "gradient in iteration367: [-39214113.83012128    140623.39564589], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5501022522964084e-13\n",
      "Iteration 368/1000: Loss = 4513.674797169861\n",
      "gradient in iteration368: [5179252.08969245  -13268.78386863], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9307807047858537e-12\n",
      "Iteration 369/1000: Loss = 4513.303728088004\n",
      "gradient in iteration369: [-3.89922276e+07  1.44031745e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5646136718340063e-13\n",
      "Iteration 370/1000: Loss = 4513.674648728748\n",
      "gradient in iteration370: [5179756.77817049  -13269.38272541], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9305925795867286e-12\n",
      "Iteration 371/1000: Loss = 4513.303437159838\n",
      "gradient in iteration371: [-39457778.00772703    334958.68478894], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.534354569596316e-13\n",
      "Iteration 372/1000: Loss = 4513.678049193059\n",
      "gradient in iteration372: [5178392.49318968  -13267.78533439], params: [0.08009705 0.04001702]\n",
      "learning_rate: 1.9311012081744335e-12\n",
      "Iteration 373/1000: Loss = 4513.304223623269\n",
      "gradient in iteration373: [-3.80907656e+07  1.37404333e+04], params: [0.08008705 0.04001704]\n",
      "learning_rate: 2.6253082195836365e-13\n",
      "Iteration 374/1000: Loss = 4513.674123233801\n",
      "gradient in iteration374: [5178899.18082801  -13268.40047113], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.930912275145156e-12\n",
      "Iteration 375/1000: Loss = 4513.303931537836\n",
      "gradient in iteration375: [-3.86412370e+07  1.41304166e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.5879088697237566e-13\n",
      "Iteration 376/1000: Loss = 4513.674429987266\n",
      "gradient in iteration376: [5179404.68130291  -13268.95735474], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307238216196762e-12\n",
      "Iteration 377/1000: Loss = 4513.303640114227\n",
      "gradient in iteration377: [-39136897.4944271      80468.73886087], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5551335543201784e-13\n",
      "Iteration 378/1000: Loss = 4513.674744607319\n",
      "gradient in iteration378: [5179521.12240891  -13269.1138333 ], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930680416908727e-12\n",
      "Iteration 379/1000: Loss = 4513.303573001363\n",
      "gradient in iteration379: [-39244765.12161163    164696.1395636 ], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5481105490151395e-13\n",
      "Iteration 380/1000: Loss = 4513.674818279244\n",
      "gradient in iteration380: [5179144.8120639   -13268.65472038], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.93082069779296e-12\n",
      "Iteration 381/1000: Loss = 4513.30378992106\n",
      "gradient in iteration381: [-3.88878608e+07  1.43202205e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.5714965540684964e-13\n",
      "Iteration 382/1000: Loss = 4513.674581806201\n",
      "gradient in iteration382: [5179649.77743759  -13269.2680836 ], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306324615922343e-12\n",
      "Iteration 383/1000: Loss = 4513.3034988399695\n",
      "gradient in iteration383: [-39361893.86913611    257685.28912522], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.540528164942048e-13\n",
      "Iteration 384/1000: Loss = 4513.676434252553\n",
      "gradient in iteration384: [5178732.58886539  -13268.1791456 ], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.9309743896606393e-12\n",
      "Iteration 385/1000: Loss = 4513.304027567067\n",
      "gradient in iteration385: [-3.84671563e+07  1.40019862e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.599620291989906e-13\n",
      "Iteration 386/1000: Loss = 4513.674328190585\n",
      "gradient in iteration386: [5179238.5160752   -13268.77968495], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9307857649270696e-12\n",
      "Iteration 387/1000: Loss = 4513.303735918486\n",
      "gradient in iteration387: [-3.89791127e+07  1.43925773e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5654765615122796e-13\n",
      "Iteration 388/1000: Loss = 4513.674640232365\n",
      "gradient in iteration388: [5179743.20764551  -13269.41323896], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9305976375893667e-12\n",
      "Iteration 389/1000: Loss = 4513.303444970304\n",
      "gradient in iteration389: [-39445710.75344356    325177.85557502], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.535129880788626e-13\n",
      "Iteration 390/1000: Loss = 4513.677844758258\n",
      "gradient in iteration390: [5178435.42661467  -13267.83312834], params: [0.08009705 0.04001702]\n",
      "learning_rate: 1.9310851977809374e-12\n",
      "Iteration 391/1000: Loss = 4513.304198877789\n",
      "gradient in iteration391: [-3.81400359e+07  1.37733201e+04], params: [0.08008705 0.04001704]\n",
      "learning_rate: 2.621916775416696e-13\n",
      "Iteration 392/1000: Loss = 4513.674148890305\n",
      "gradient in iteration392: [5178942.0080695   -13268.39592366], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.930896307473348e-12\n",
      "Iteration 393/1000: Loss = 4513.303906845341\n",
      "gradient in iteration393: [-3.86850567e+07  1.41634106e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.5849774688074555e-13\n",
      "Iteration 394/1000: Loss = 4513.6744563131215\n",
      "gradient in iteration394: [5179447.45115528  -13269.06008217], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307078784570902e-12\n",
      "Iteration 395/1000: Loss = 4513.3036154815345\n",
      "gradient in iteration395: [-39176705.91808929    111392.44429614], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5525372196702846e-13\n",
      "Iteration 396/1000: Loss = 4513.674771596542\n",
      "gradient in iteration396: [5179382.63233191  -13268.97399849], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930732040837406e-12\n",
      "Iteration 397/1000: Loss = 4513.303652829464\n",
      "gradient in iteration397: [-39116251.07371037     64504.70918935], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5564822102087635e-13\n",
      "Iteration 398/1000: Loss = 4513.674730702512\n",
      "gradient in iteration398: [5179592.7708099   -13269.19805251], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930653710144158e-12\n",
      "Iteration 399/1000: Loss = 4513.303531704727\n",
      "gradient in iteration399: [-39310244.21622169    216487.31380992], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.54386615992414e-13\n",
      "Iteration 400/1000: Loss = 4513.675573434778\n",
      "gradient in iteration400: [5178914.83723361  -13268.38068962], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9309064377937617e-12\n",
      "Iteration 401/1000: Loss = 4513.303922513823\n",
      "gradient in iteration401: [-3.86572923e+07  1.41426388e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.5868340496316305e-13\n",
      "Iteration 402/1000: Loss = 4513.674439600084\n",
      "gradient in iteration402: [5179420.28222933  -13268.99559899], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307180060884715e-12\n",
      "Iteration 403/1000: Loss = 4513.303631112864\n",
      "gradient in iteration403: [-39151473.21772565     91769.77737297], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5541823022569037e-13\n",
      "Iteration 404/1000: Loss = 4513.674754461733\n",
      "gradient in iteration404: [5179470.46787413  -13269.06885879], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930699298707349e-12\n",
      "Iteration 405/1000: Loss = 4513.3036021997095\n",
      "gradient in iteration405: [-39198063.21960095    128060.86365345], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.55114645434816e-13\n",
      "Iteration 406/1000: Loss = 4513.674786171745\n",
      "gradient in iteration406: [5179308.17136403  -13268.87577306], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307597982466433e-12\n",
      "Iteration 407/1000: Loss = 4513.303695767606\n",
      "gradient in iteration407: [-3.90459993e+07  1.44465885e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.561081844446711e-13\n",
      "Iteration 408/1000: Loss = 4513.674683861523\n",
      "gradient in iteration408: [5179812.70149903  -13269.45880464], params: [0.08009705 0.04001708]\n",
      "learning_rate: 1.930571736137489e-12\n",
      "Iteration 409/1000: Loss = 4513.303404918988\n",
      "gradient in iteration409: [-39507375.1658856     347267.41923805], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.531172966569276e-13\n",
      "Iteration 410/1000: Loss = 4513.678893000759\n",
      "gradient in iteration410: [5178379.16690951  -13267.75550297], params: [0.08009705 0.04001701]\n",
      "learning_rate: 1.9311061777594915e-12\n",
      "Iteration 411/1000: Loss = 4513.304231311433\n",
      "gradient in iteration411: [-3.80753433e+07  1.37301769e+04], params: [0.08008705 0.04001704]\n",
      "learning_rate: 2.626371592196298e-13\n",
      "Iteration 412/1000: Loss = 4513.674115275709\n",
      "gradient in iteration412: [5178885.86841764  -13268.35331383], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9309172385865712e-12\n",
      "Iteration 413/1000: Loss = 4513.303939209835\n",
      "gradient in iteration413: [-3.8627545e+07  1.4120116e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.5888261846779035e-13\n",
      "Iteration 414/1000: Loss = 4513.674421821574\n",
      "gradient in iteration414: [5179391.42155228  -13268.93670921], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307287644622476e-12\n",
      "Iteration 415/1000: Loss = 4513.303647768273\n",
      "gradient in iteration415: [-39124475.74011821     70857.86101015], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5559447918035633e-13\n",
      "Iteration 416/1000: Loss = 4513.674736232604\n",
      "gradient in iteration416: [5179564.25046577  -13269.13661615], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306643409434974e-12\n",
      "Iteration 417/1000: Loss = 4513.303548146922\n",
      "gradient in iteration417: [-39284252.78086252    195869.76432038], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.545549244829099e-13\n",
      "Iteration 418/1000: Loss = 4513.675142683206\n",
      "gradient in iteration418: [5179006.30688928  -13268.50342593], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9308723348526694e-12\n",
      "Iteration 419/1000: Loss = 4513.303869801256\n",
      "gradient in iteration419: [-3.87501296e+07  1.42132179e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.5806365293514307e-13\n",
      "Iteration 420/1000: Loss = 4513.674495923616\n",
      "gradient in iteration420: [5179511.55484297  -13269.09464296], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930683983251229e-12\n",
      "Iteration 421/1000: Loss = 4513.303578526409\n",
      "gradient in iteration421: [-39235954.41970064    157764.93560224], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5486827446661857e-13\n",
      "Iteration 422/1000: Loss = 4513.674812196272\n",
      "gradient in iteration422: [5179175.68868143  -13268.72443315], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9308091868468584e-12\n",
      "Iteration 423/1000: Loss = 4513.303772130725\n",
      "gradient in iteration423: [-3.89180833e+07  1.43440371e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.5694996162033604e-13\n",
      "Iteration 424/1000: Loss = 4513.674601021941\n",
      "gradient in iteration424: [5179680.5541926   -13269.34366261], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306209901121563e-12\n",
      "Iteration 425/1000: Loss = 4513.303481092973\n",
      "gradient in iteration425: [-39389622.8984886     279926.79456474], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.5387397147139753e-13\n",
      "Iteration 426/1000: Loss = 4513.676899025198\n",
      "gradient in iteration426: [5178634.47839773  -13268.05345343], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.93101097243187e-12\n",
      "Iteration 427/1000: Loss = 4513.304084128295\n",
      "gradient in iteration427: [-3.83616704e+07  1.39264645e+04], params: [0.08008705 0.04001705]\n",
      "learning_rate: 2.606768656562879e-13\n",
      "Iteration 428/1000: Loss = 4513.674268664564\n",
      "gradient in iteration428: [5179140.6052871   -13268.65531155], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9308222661094674e-12\n",
      "Iteration 429/1000: Loss = 4513.303792351252\n",
      "gradient in iteration429: [-3.88837203e+07  1.43169731e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.5717703743283644e-13\n",
      "Iteration 430/1000: Loss = 4513.674579183745\n",
      "gradient in iteration430: [5179645.56242987  -13269.26585534], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306340326709177e-12\n",
      "Iteration 431/1000: Loss = 4513.303501264383\n",
      "gradient in iteration431: [-39358097.77908617    254646.71612099], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.5407731989816166e-13\n",
      "Iteration 432/1000: Loss = 4513.676370758868\n",
      "gradient in iteration432: [5178745.99349852  -13268.1652758 ], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.9309693915388304e-12\n",
      "Iteration 433/1000: Loss = 4513.304019831158\n",
      "gradient in iteration433: [-3.84814055e+07  1.40124528e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.5986576818454633e-13\n",
      "Iteration 434/1000: Loss = 4513.674336355032\n",
      "gradient in iteration434: [5179251.88792105  -13268.80133092], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9307807800045013e-12\n",
      "Iteration 435/1000: Loss = 4513.303728201442\n",
      "gradient in iteration435: [-3.89920373e+07  1.44030463e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.564626186806056e-13\n",
      "Iteration 436/1000: Loss = 4513.674648604438\n",
      "gradient in iteration436: [5179756.54242946  -13269.38077   ], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9305926674518385e-12\n",
      "Iteration 437/1000: Loss = 4513.30343727297\n",
      "gradient in iteration437: [-39457602.87310804    334816.3040755 ], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.534365818460656e-13\n",
      "Iteration 438/1000: Loss = 4513.678046219599\n",
      "gradient in iteration438: [5178393.11210082  -13267.76564383], params: [0.08009705 0.04001702]\n",
      "learning_rate: 1.9311009773730954e-12\n",
      "Iteration 439/1000: Loss = 4513.304223263337\n",
      "gradient in iteration439: [-3.80914860e+07  1.37408036e+04], params: [0.08008705 0.04001704]\n",
      "learning_rate: 2.6252585686222026e-13\n",
      "Iteration 440/1000: Loss = 4513.674123607077\n",
      "gradient in iteration440: [5178899.79792017  -13268.37991655], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9309120450671737e-12\n",
      "Iteration 441/1000: Loss = 4513.3039311788025\n",
      "gradient in iteration441: [-3.86418766e+07  1.41308889e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.587866038500304e-13\n",
      "Iteration 442/1000: Loss = 4513.6744303694\n",
      "gradient in iteration442: [5179405.32467947  -13268.96708633], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930723581788584e-12\n",
      "Iteration 443/1000: Loss = 4513.303639756007\n",
      "gradient in iteration443: [-39137478.47554987     80918.6812603 ], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5550956243252216e-13\n",
      "Iteration 444/1000: Loss = 4513.674744999162\n",
      "gradient in iteration444: [5179519.10846932  -13269.12333752], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930681167610414e-12\n",
      "Iteration 445/1000: Loss = 4513.303574164658\n",
      "gradient in iteration445: [-39242911.5795494     163236.77123182], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5482309027272293e-13\n",
      "Iteration 446/1000: Loss = 4513.674816997897\n",
      "gradient in iteration446: [5179151.30180882  -13268.68268734], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9308182783745874e-12\n",
      "Iteration 447/1000: Loss = 4513.303786175727\n",
      "gradient in iteration447: [-3.88942370e+07  1.43252053e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.571074987290399e-13\n",
      "Iteration 448/1000: Loss = 4513.674585847954\n",
      "gradient in iteration448: [5179656.21311305  -13269.28777416], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930630062799063e-12\n",
      "Iteration 449/1000: Loss = 4513.30349510326\n",
      "gradient in iteration449: [-39367741.92270605    262368.38325531], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.540150771063738e-13\n",
      "Iteration 450/1000: Loss = 4513.67653212436\n",
      "gradient in iteration450: [5178711.9146411   -13268.15636276], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.9309820984110546e-12\n",
      "Iteration 451/1000: Loss = 4513.3040394847385\n",
      "gradient in iteration451: [-3.84451205e+07  1.39860619e+04], params: [0.08008705 0.04001705]\n",
      "learning_rate: 2.601110325993333e-13\n",
      "Iteration 452/1000: Loss = 4513.674315621376\n",
      "gradient in iteration452: [5179217.87477462  -13268.75271843], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9307934598976798e-12\n",
      "Iteration 453/1000: Loss = 4513.303747809206\n",
      "gradient in iteration453: [-3.89591432e+07  1.43766828e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5667915608032437e-13\n",
      "Iteration 454/1000: Loss = 4513.674627342245\n",
      "gradient in iteration454: [5179722.66056859  -13269.37167505], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306052959411315e-12\n",
      "Iteration 455/1000: Loss = 4513.303456831424\n",
      "gradient in iteration455: [-39427343.20012277    310320.8669927 ], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.536310891972265e-13\n",
      "Iteration 456/1000: Loss = 4513.6775342258\n",
      "gradient in iteration456: [5178500.67985819  -13267.91107203], params: [0.08009705 0.04001702]\n",
      "learning_rate: 1.9310608645655033e-12\n",
      "Iteration 457/1000: Loss = 4513.304161251495\n",
      "gradient in iteration457: [-3.82139123e+07  1.38234773e+04], params: [0.08008705 0.04001705]\n",
      "learning_rate: 2.61684799993808e-13\n",
      "Iteration 458/1000: Loss = 4513.6741880196505\n",
      "gradient in iteration458: [5179007.15626638  -13268.48300778], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9308720181821764e-12\n",
      "Iteration 459/1000: Loss = 4513.303869300808\n",
      "gradient in iteration459: [-3.87510013e+07  1.42137718e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.5805784768870907e-13\n",
      "Iteration 460/1000: Loss = 4513.674496459066\n",
      "gradient in iteration460: [5179512.38211936  -13269.07090515], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930683674880645e-12\n",
      "Iteration 461/1000: Loss = 4513.303578027265\n",
      "gradient in iteration461: [-39236749.0162087    158389.9500929], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.548631130440751e-13\n",
      "Iteration 462/1000: Loss = 4513.674812744231\n",
      "gradient in iteration462: [5179172.91108461  -13268.702196  ], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.930810222342206e-12\n",
      "Iteration 463/1000: Loss = 4513.303773735587\n",
      "gradient in iteration463: [-3.89153630e+07  1.43419723e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.569679232489988e-13\n",
      "Iteration 464/1000: Loss = 4513.674599288185\n",
      "gradient in iteration464: [5179677.78150705  -13269.28859271], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930622023575075e-12\n",
      "Iteration 465/1000: Loss = 4513.303482694231\n",
      "gradient in iteration465: [-39387123.29891132    277918.31503237], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.5389008291134595e-13\n",
      "Iteration 466/1000: Loss = 4513.67685705418\n",
      "gradient in iteration466: [5178643.34760816  -13268.09720013], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.931007665283354e-12\n",
      "Iteration 467/1000: Loss = 4513.304079024909\n",
      "gradient in iteration467: [-3.83712854e+07  1.39332543e+04], params: [0.08008705 0.04001705]\n",
      "learning_rate: 2.60611545486647e-13\n",
      "Iteration 468/1000: Loss = 4513.674274023597\n",
      "gradient in iteration468: [5179149.45571647  -13268.68109572], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.930818966608993e-12\n",
      "Iteration 469/1000: Loss = 4513.3037872587\n",
      "gradient in iteration469: [-3.88923945e+07  1.43236906e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.5711967919607135e-13\n",
      "Iteration 470/1000: Loss = 4513.67458468\n",
      "gradient in iteration470: [5179654.34819416  -13269.24843852], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930630757916579e-12\n",
      "Iteration 471/1000: Loss = 4513.303496183519\n",
      "gradient in iteration471: [-39366051.13899532    261014.22249667], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.54025987130169e-13\n",
      "Iteration 472/1000: Loss = 4513.676503815557\n",
      "gradient in iteration472: [5178717.92108053  -13268.14067397], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.930979858797467e-12\n",
      "Iteration 473/1000: Loss = 4513.304036038932\n",
      "gradient in iteration473: [-3.84515009e+07  1.39907475e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.6006787188350905e-13\n",
      "Iteration 474/1000: Loss = 4513.6743192528165\n",
      "gradient in iteration474: [5179223.85161     -13268.77854808], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9307912317579053e-12\n",
      "Iteration 475/1000: Loss = 4513.303744372284\n",
      "gradient in iteration475: [-3.89649223e+07  1.43812978e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5664108684128673e-13\n",
      "Iteration 476/1000: Loss = 4513.674631067232\n",
      "gradient in iteration476: [5179728.57064706  -13269.35075667], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930603093117443e-12\n",
      "Iteration 477/1000: Loss = 4513.303453403086\n",
      "gradient in iteration477: [-39432655.77649982    314614.67078771], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.535969186726595e-13\n",
      "Iteration 478/1000: Loss = 4513.6776239781\n",
      "gradient in iteration478: [5178481.80584224  -13267.87510152], params: [0.08009705 0.04001702]\n",
      "learning_rate: 1.9310679027042717e-12\n",
      "Iteration 479/1000: Loss = 4513.304172130541\n",
      "gradient in iteration479: [-3.81926767e+07  1.38090877e+04], params: [0.08008705 0.04001704]\n",
      "learning_rate: 2.6183030038282865e-13\n",
      "Iteration 480/1000: Loss = 4513.6741766889445\n",
      "gradient in iteration480: [5178988.35155393  -13268.47705059], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9308790291060525e-12\n",
      "Iteration 481/1000: Loss = 4513.303880157304\n",
      "gradient in iteration481: [-3.87320167e+07  1.41992746e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.581843354041501e-13\n",
      "Iteration 482/1000: Loss = 4513.674484836603\n",
      "gradient in iteration482: [5179493.59773411  -13269.07790826], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9306906768597485e-12\n",
      "Iteration 483/1000: Loss = 4513.303588856943\n",
      "gradient in iteration483: [-39219446.40505899    144803.26699413], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5497555209525046e-13\n",
      "Iteration 484/1000: Loss = 4513.674800830372\n",
      "gradient in iteration484: [5179233.45778399  -13268.76081293], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9307876506263223e-12\n",
      "Iteration 485/1000: Loss = 4513.303738833359\n",
      "gradient in iteration485: [-3.89742237e+07  1.43887375e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.565798381339128e-13\n",
      "Iteration 486/1000: Loss = 4513.674637073321\n",
      "gradient in iteration486: [5179738.19164581  -13269.37290287], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930599507158218e-12\n",
      "Iteration 487/1000: Loss = 4513.303447878454\n",
      "gradient in iteration487: [-39441211.08938816    321534.38660316], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.53541910194805e-13\n",
      "Iteration 488/1000: Loss = 4513.677768617777\n",
      "gradient in iteration488: [5178451.40020654  -13267.86118625], params: [0.08009705 0.04001702]\n",
      "learning_rate: 1.9310792411030746e-12\n",
      "Iteration 489/1000: Loss = 4513.304189654226\n",
      "gradient in iteration489: [-3.81582598e+07  1.37856097e+04], params: [0.08008705 0.04001704]\n",
      "learning_rate: 2.620664579739805e-13\n",
      "Iteration 490/1000: Loss = 4513.674158469102\n",
      "gradient in iteration490: [5178957.98020618  -13268.4440814 ], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.930890352503284e-12\n",
      "Iteration 491/1000: Loss = 4513.303897642026\n",
      "gradient in iteration491: [-3.87012988e+07  1.41758881e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.5838926127837616e-13\n",
      "Iteration 492/1000: Loss = 4513.674466140978\n",
      "gradient in iteration492: [5179463.33879991  -13269.04080088], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930701956144558e-12\n",
      "Iteration 493/1000: Loss = 4513.303606299551\n",
      "gradient in iteration493: [-39191477.22963317    122914.99310777], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.551575165541062e-13\n",
      "Iteration 494/1000: Loss = 4513.674781668807\n",
      "gradient in iteration494: [5179331.15183951  -13268.9081056 ], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930751231546253e-12\n",
      "Iteration 495/1000: Loss = 4513.303682518357\n",
      "gradient in iteration495: [-3.90677642e+07  2.72158295e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.559655050570886e-13\n",
      "Iteration 496/1000: Loss = 4513.674698297091\n",
      "gradient in iteration496: [5179760.48117816  -13269.4021886 ], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930591199407245e-12\n",
      "Iteration 497/1000: Loss = 4513.303435013412\n",
      "gradient in iteration497: [-39461090.27233433    337645.55450944], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.534141842251853e-13\n",
      "Iteration 498/1000: Loss = 4513.678105362625\n",
      "gradient in iteration498: [5178380.69722529  -13267.74322479], params: [0.08009705 0.04001701]\n",
      "learning_rate: 1.931105607078725e-12\n",
      "Iteration 499/1000: Loss = 4513.304230417095\n",
      "gradient in iteration499: [-3.80771390e+07  1.37313799e+04], params: [0.08008705 0.04001704]\n",
      "learning_rate: 2.6262477360333176e-13\n",
      "Iteration 500/1000: Loss = 4513.674116199616\n",
      "gradient in iteration500: [5178887.40396302  -13268.35995314], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.930916666067647e-12\n",
      "Iteration 501/1000: Loss = 4513.30393831772\n",
      "gradient in iteration501: [-3.86291392e+07  1.41213889e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.588719345860591e-13\n",
      "Iteration 502/1000: Loss = 4513.674422770269\n",
      "gradient in iteration502: [5179392.94813914  -13268.9684051 ], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307281953945624e-12\n",
      "Iteration 503/1000: Loss = 4513.303646879056\n",
      "gradient in iteration503: [-39125921.83269226     71976.05309557], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5558503241818444e-13\n",
      "Iteration 504/1000: Loss = 4513.674737207687\n",
      "gradient in iteration504: [5179559.24924536  -13269.14771198], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306662051326013e-12\n",
      "Iteration 505/1000: Loss = 4513.303551039711\n",
      "gradient in iteration505: [-39279668.67718169    192241.9030052 ], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5458463212061644e-13\n",
      "Iteration 506/1000: Loss = 4513.6750668991945\n",
      "gradient in iteration506: [5179022.37197632  -13268.53284809], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9308663453762992e-12\n",
      "Iteration 507/1000: Loss = 4513.303860515648\n",
      "gradient in iteration507: [-3.87663173e+07  1.42256232e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.5795589280878155e-13\n",
      "Iteration 508/1000: Loss = 4513.674505873028\n",
      "gradient in iteration508: [5179527.60310436  -13269.13370576], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9306780012151066e-12\n",
      "Iteration 509/1000: Loss = 4513.303569262835\n",
      "gradient in iteration509: [-39250721.10332622    169386.1462627 ], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.547723893702573e-13\n",
      "Iteration 510/1000: Loss = 4513.6748223973345\n",
      "gradient in iteration510: [5179123.95785061  -13268.63493887], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9308284724179708e-12\n",
      "Iteration 511/1000: Loss = 4513.303801952227\n",
      "gradient in iteration511: [-3.88673319e+07  1.43040219e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.572854763241569e-13\n",
      "Iteration 512/1000: Loss = 4513.674568829033\n",
      "gradient in iteration512: [5179628.94514312  -13269.26653746], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306402265314555e-12\n",
      "Iteration 513/1000: Loss = 4513.303510841836\n",
      "gradient in iteration513: [-39343080.23835911    242643.57798529], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.541743030646111e-13\n",
      "Iteration 514/1000: Loss = 4513.676119935801\n",
      "gradient in iteration514: [5178799.06665257  -13268.24490206], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.930949602658308e-12\n",
      "Iteration 515/1000: Loss = 4513.3039892544875\n",
      "gradient in iteration515: [-3.85373310e+07  1.40531874e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.594886498645236e-13\n",
      "Iteration 516/1000: Loss = 4513.674368691182\n",
      "gradient in iteration516: [5179304.77617483  -13268.84494119], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.930761063917441e-12\n",
      "Iteration 517/1000: Loss = 4513.303697695012\n",
      "gradient in iteration517: [-3.90428244e+07  1.44439164e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5612901105923416e-13\n",
      "Iteration 518/1000: Loss = 4513.674681764274\n",
      "gradient in iteration518: [5179809.34109801  -13269.44893663], params: [0.08009705 0.04001708]\n",
      "learning_rate: 1.9305729885957962e-12\n",
      "Iteration 519/1000: Loss = 4513.303406842405\n",
      "gradient in iteration519: [-39504425.71340318    347272.53860183], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.5313619472785223e-13\n",
      "Iteration 520/1000: Loss = 4513.678842655411\n",
      "gradient in iteration520: [5178375.66294475  -13267.74827248], params: [0.08009705 0.04001701]\n",
      "learning_rate: 1.9311074844487783e-12\n",
      "Iteration 521/1000: Loss = 4513.304233338926\n",
      "gradient in iteration521: [-3.80712666e+07  1.37274945e+04], params: [0.08008705 0.04001704]\n",
      "learning_rate: 2.6266528288402955e-13\n",
      "Iteration 522/1000: Loss = 4513.674113179319\n",
      "gradient in iteration522: [5178882.35085593  -13268.37555098], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9309185500897257e-12\n",
      "Iteration 523/1000: Loss = 4513.303941233217\n",
      "gradient in iteration523: [-3.86239295e+07  1.41174388e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.589068517964214e-13\n",
      "Iteration 524/1000: Loss = 4513.674419668547\n",
      "gradient in iteration524: [5179387.91122106  -13268.9590373 ], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930730073014066e-12\n",
      "Iteration 525/1000: Loss = 4513.303649786667\n",
      "gradient in iteration525: [-39121197.79970453     68324.78848082], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5561589528006545e-13\n",
      "Iteration 526/1000: Loss = 4513.674734028314\n",
      "gradient in iteration526: [5179575.60095965  -13269.16731159], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930660110096132e-12\n",
      "Iteration 527/1000: Loss = 4513.303541591867\n",
      "gradient in iteration527: [-39294627.12353598    204089.88339113], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5448771834789556e-13\n",
      "Iteration 528/1000: Loss = 4513.675314410578\n",
      "gradient in iteration528: [5178969.7949971   -13268.49715042], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.930885947560464e-12\n",
      "Iteration 529/1000: Loss = 4513.303890829211\n",
      "gradient in iteration529: [-3.87132907e+07  1.41849868e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.5830922214296895e-13\n",
      "Iteration 530/1000: Loss = 4513.674473422701\n",
      "gradient in iteration530: [5179475.13417312  -13269.05448877], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930697559299406e-12\n",
      "Iteration 531/1000: Loss = 4513.303599502293\n",
      "gradient in iteration531: [-39202391.69724482    131445.43881936], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5508647730548566e-13\n",
      "Iteration 532/1000: Loss = 4513.6747891332725\n",
      "gradient in iteration532: [5179293.0407828   -13268.83166257], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9307654386917266e-12\n",
      "Iteration 533/1000: Loss = 4513.303704478788\n",
      "gradient in iteration533: [-3.90315935e+07  1.44349343e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5620270920369017e-13\n",
      "Iteration 534/1000: Loss = 4513.674674380405\n",
      "gradient in iteration534: [5179797.63990297  -13269.45052824], params: [0.08009705 0.04001708]\n",
      "learning_rate: 1.9305773497721664e-12\n",
      "Iteration 535/1000: Loss = 4513.303413609032\n",
      "gradient in iteration535: [-39494041.17861377    347291.50111161], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.53202754176877e-13\n",
      "Iteration 536/1000: Loss = 4513.678665593736\n",
      "gradient in iteration536: [5178363.2551178  -13267.7324018], params: [0.08009705 0.04001701]\n",
      "learning_rate: 1.9311121115570543e-12\n",
      "Iteration 537/1000: Loss = 4513.304240474489\n",
      "gradient in iteration537: [-3.80568902e+07  1.37179097e+04], params: [0.08008705 0.04001704]\n",
      "learning_rate: 2.6276450723996895e-13\n",
      "Iteration 538/1000: Loss = 4513.674105798921\n",
      "gradient in iteration538: [5178870.00278279  -13268.36450062], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.930923154013646e-12\n",
      "Iteration 539/1000: Loss = 4513.303948353121\n",
      "gradient in iteration539: [-3.86111826e+07  1.41079728e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.5899232653506245e-13\n",
      "Iteration 540/1000: Loss = 4513.674412095372\n",
      "gradient in iteration540: [5179375.59225175  -13268.93639089], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.93073466519011e-12\n",
      "Iteration 541/1000: Loss = 4513.303656889163\n",
      "gradient in iteration541: [-39109641.61050288     59404.84784333], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5569142513733764e-13\n",
      "Iteration 542/1000: Loss = 4513.674726262544\n",
      "gradient in iteration542: [5179615.65133191  -13269.22656517], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.93064518164172e-12\n",
      "Iteration 543/1000: Loss = 4513.303518500014\n",
      "gradient in iteration543: [-39331045.71010517    233042.22249863], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.542520754140727e-13\n",
      "Iteration 544/1000: Loss = 4513.675919335563\n",
      "gradient in iteration544: [5178841.5307783   -13268.28905803], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.9309337697570283e-12\n",
      "Iteration 545/1000: Loss = 4513.303964772038\n",
      "gradient in iteration545: [-3.85816637e+07  1.40859714e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.591904817444658e-13\n",
      "Iteration 546/1000: Loss = 4513.674394651735\n",
      "gradient in iteration546: [5179347.18577635  -13268.92111138], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307452544332698e-12\n",
      "Iteration 547/1000: Loss = 4513.303673270717\n",
      "gradient in iteration547: [-3.90829080e+07  3.88314864e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5586632406034716e-13\n",
      "Iteration 548/1000: Loss = 4513.674708379353\n",
      "gradient in iteration548: [5179708.18150388  -13269.33092969], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306106926465085e-12\n",
      "Iteration 549/1000: Loss = 4513.303465168134\n",
      "gradient in iteration549: [-39414403.5235513     299876.48667884], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.537143558198134e-13\n",
      "Iteration 550/1000: Loss = 4513.677315953882\n",
      "gradient in iteration550: [5178546.63462458  -13267.98901573], params: [0.08009705 0.04001702]\n",
      "learning_rate: 1.9310437282032803e-12\n",
      "Iteration 551/1000: Loss = 4513.304134771351\n",
      "gradient in iteration551: [-3.82651838e+07  1.38587560e+04], params: [0.08008705 0.04001705]\n",
      "learning_rate: 2.6133416890273295e-13\n",
      "Iteration 552/1000: Loss = 4513.6742156410555\n",
      "gradient in iteration552: [5179052.96701429  -13268.56349806], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9308549388644276e-12\n",
      "Iteration 553/1000: Loss = 4513.3038428791615\n",
      "gradient in iteration553: [-3.87969329e+07  1.42492294e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.577523339378543e-13\n",
      "Iteration 554/1000: Loss = 4513.674524796543\n",
      "gradient in iteration554: [5179558.12874789  -13269.17181359], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930666622794987e-12\n",
      "Iteration 555/1000: Loss = 4513.303551668524\n",
      "gradient in iteration555: [-39278672.89262395    191453.67614328], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.545910862960413e-13\n",
      "Iteration 556/1000: Loss = 4513.6750504315705\n",
      "gradient in iteration556: [5179025.89776895  -13268.52070633], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9308650308753724e-12\n",
      "Iteration 557/1000: Loss = 4513.303858497865\n",
      "gradient in iteration557: [-3.87698278e+07  1.42283632e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.5793253591304475e-13\n",
      "Iteration 558/1000: Loss = 4513.674508035839\n",
      "gradient in iteration558: [5179531.06232199  -13269.13593402], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9306767117865298e-12\n",
      "Iteration 559/1000: Loss = 4513.303567249857\n",
      "gradient in iteration559: [-39253923.07469337    171910.04744745], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5475160739913165e-13\n",
      "Iteration 560/1000: Loss = 4513.674824612699\n",
      "gradient in iteration560: [5179112.70221703  -13268.64389739], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9308326686382582e-12\n",
      "Iteration 561/1000: Loss = 4513.303808425309\n",
      "gradient in iteration561: [-3.88562577e+07  1.42954064e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.5735880381716786e-13\n",
      "Iteration 562/1000: Loss = 4513.6745618543055\n",
      "gradient in iteration562: [5179617.75326512  -13269.23238594], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930644398169385e-12\n",
      "Iteration 563/1000: Loss = 4513.303517298461\n",
      "gradient in iteration563: [-39332935.21763517    234549.27122657], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.542398614461002e-13\n",
      "Iteration 564/1000: Loss = 4513.6759508107325\n",
      "gradient in iteration564: [5178834.87268591  -13268.31061306], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.9309362522334443e-12\n",
      "Iteration 565/1000: Loss = 4513.303968617014\n",
      "gradient in iteration565: [-3.85747293e+07  1.40809171e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.592370752487066e-13\n",
      "Iteration 566/1000: Loss = 4513.674390571024\n",
      "gradient in iteration566: [5179340.53486897  -13268.93184341], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307477337465677e-12\n",
      "Iteration 567/1000: Loss = 4513.303677106246\n",
      "gradient in iteration567: [-3.90766332e+07  3.40152634e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.559074100096384e-13\n",
      "Iteration 568/1000: Loss = 4513.674704196173\n",
      "gradient in iteration568: [5179729.87831854  -13269.37399426], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.93060260571855e-12\n",
      "Iteration 569/1000: Loss = 4513.3034526695765\n",
      "gradient in iteration569: [-39433793.83918      315534.1530146], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.5358959984378575e-13\n",
      "Iteration 570/1000: Loss = 4513.677643196161\n",
      "gradient in iteration570: [5178477.78992291  -13267.91243628], params: [0.08009705 0.04001702]\n",
      "learning_rate: 1.9310694002510863e-12\n",
      "Iteration 571/1000: Loss = 4513.304174459529\n",
      "gradient in iteration571: [-3.81881201e+07  1.38059778e+04], params: [0.08008705 0.04001704]\n",
      "learning_rate: 2.618615417924e-13\n",
      "Iteration 572/1000: Loss = 4513.674174266931\n",
      "gradient in iteration572: [5178984.24550367  -13268.47645942], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9308805599634477e-12\n",
      "Iteration 573/1000: Loss = 4513.303882479609\n",
      "gradient in iteration573: [-3.87279467e+07  1.41961601e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.582114689996028e-13\n",
      "Iteration 574/1000: Loss = 4513.674482349526\n",
      "gradient in iteration574: [5179489.6039155  -13269.0879127], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9306921655833377e-12\n",
      "Iteration 575/1000: Loss = 4513.303591174021\n",
      "gradient in iteration575: [-39215738.81484619    141896.45871738], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5499965835692043e-13\n",
      "Iteration 576/1000: Loss = 4513.674798284337\n",
      "gradient in iteration576: [5179246.41485479  -13268.7792302 ], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9307828203189235e-12\n",
      "Iteration 577/1000: Loss = 4513.303731360696\n",
      "gradient in iteration577: [-3.89867493e+07  1.43987914e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5649740405683767e-13\n",
      "Iteration 578/1000: Loss = 4513.674645176593\n",
      "gradient in iteration578: [5179751.11156375  -13269.36012447], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930594691639738e-12\n",
      "Iteration 579/1000: Loss = 4513.303440424105\n",
      "gradient in iteration579: [-39452735.28934194    330869.04445554], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.534678502431104e-13\n",
      "Iteration 580/1000: Loss = 4513.6779637266945\n",
      "gradient in iteration580: [5178410.44070337  -13267.80111413], params: [0.08009705 0.04001702]\n",
      "learning_rate: 1.9310945152971922e-12\n",
      "Iteration 581/1000: Loss = 4513.304213278644\n",
      "gradient in iteration581: [-3.81114301e+07  1.37541965e+04], params: [0.08008705 0.04001704]\n",
      "learning_rate: 2.6238847431196106e-13\n",
      "Iteration 582/1000: Loss = 4513.674133952359\n",
      "gradient in iteration582: [5178917.06267619  -13268.40588263], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9309056080601777e-12\n",
      "Iteration 583/1000: Loss = 4513.303921215553\n",
      "gradient in iteration583: [-3.86596001e+07  1.41442240e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.5866796261968343e-13\n",
      "Iteration 584/1000: Loss = 4513.6744409852345\n",
      "gradient in iteration584: [5179422.5267713   -13268.99978267], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930717169397203e-12\n",
      "Iteration 585/1000: Loss = 4513.303629816747\n",
      "gradient in iteration585: [-39153570.18304349     93397.39908683], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5540455067698453e-13\n",
      "Iteration 586/1000: Loss = 4513.6747558804955\n",
      "gradient in iteration586: [5179463.19000657  -13269.03434347], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930702011608912e-12\n",
      "Iteration 587/1000: Loss = 4513.303606402706\n",
      "gradient in iteration587: [-39191311.46535619    122785.54881959], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5515859577293465e-13\n",
      "Iteration 588/1000: Loss = 4513.674781556299\n",
      "gradient in iteration588: [5179331.72627636  -13268.88941549], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307510174076886e-12\n",
      "Iteration 589/1000: Loss = 4513.303682184624\n",
      "gradient in iteration589: [-3.90683102e+07  2.76345552e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.559619280270893e-13\n",
      "Iteration 590/1000: Loss = 4513.674698657932\n",
      "gradient in iteration590: [5179758.60698243  -13269.39636783], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9305918979544284e-12\n",
      "Iteration 591/1000: Loss = 4513.303436101648\n",
      "gradient in iteration591: [-39459411.21128272    336283.36154834], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.5342496740399103e-13\n",
      "Iteration 592/1000: Loss = 4513.678076893298\n",
      "gradient in iteration592: [5178386.68824879  -13267.78119619], params: [0.08009705 0.04001702]\n",
      "learning_rate: 1.931103372927479e-12\n",
      "Iteration 593/1000: Loss = 4513.30422697301\n",
      "gradient in iteration593: [-3.80840536e+07  1.37358973e+04], params: [0.08008705 0.04001704]\n",
      "learning_rate: 2.6257709048287676e-13\n",
      "Iteration 594/1000: Loss = 4513.67411976578\n",
      "gradient in iteration594: [5178893.38912028  -13268.38983005], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.930914434540749e-12\n",
      "Iteration 595/1000: Loss = 4513.30393487978\n",
      "gradient in iteration595: [-3.86352771e+07  1.41260054e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.5883080824411125e-13\n",
      "Iteration 596/1000: Loss = 4513.674426429566\n",
      "gradient in iteration596: [5179398.92520189  -13268.94202975], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307259673206574e-12\n",
      "Iteration 597/1000: Loss = 4513.303643448916\n",
      "gradient in iteration597: [-39131488.93698235     76282.18659192], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5554867120195907e-13\n",
      "Iteration 598/1000: Loss = 4513.67474095838\n",
      "gradient in iteration598: [5179539.86704924  -13269.14189121], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.93067342981896e-12\n",
      "Iteration 599/1000: Loss = 4513.303562177416\n",
      "gradient in iteration599: [-39261991.73092755    178273.50266089], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5469925388738687e-13\n",
      "Iteration 600/1000: Loss = 4513.674830202235\n",
      "gradient in iteration600: [5179084.4043353  -13268.5862809], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9308432184710522e-12\n",
      "Iteration 601/1000: Loss = 4513.3038247382065\n",
      "gradient in iteration601: [-3.88282464e+07  1.42734998e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.5754446625291524e-13\n",
      "Iteration 602/1000: Loss = 4513.674544292255\n",
      "gradient in iteration602: [5179589.52414119  -13269.19837084], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306549203159226e-12\n",
      "Iteration 603/1000: Loss = 4513.303533572763\n",
      "gradient in iteration603: [-39307298.16697749    214146.28176899], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5440568205731107e-13\n",
      "Iteration 604/1000: Loss = 4513.675524523141\n",
      "gradient in iteration604: [5178925.20392707  -13268.39169451], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9309025726838495e-12\n",
      "Iteration 605/1000: Loss = 4513.303916533555\n",
      "gradient in iteration605: [-3.86679076e+07  1.41505248e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.586123903110853e-13\n",
      "Iteration 606/1000: Loss = 4513.674445975464\n",
      "gradient in iteration606: [5179430.65992768  -13269.02192886], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930714137630648e-12\n",
      "Iteration 607/1000: Loss = 4513.303625146709\n",
      "gradient in iteration607: [-39161117.43214969     99260.26973517], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5535532833877733e-13\n",
      "Iteration 608/1000: Loss = 4513.674761000089\n",
      "gradient in iteration608: [5179436.93303096  -13269.04703092], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307117992356938e-12\n",
      "Iteration 609/1000: Loss = 4513.303621536547\n",
      "gradient in iteration609: [-39166944.28212395    103791.99311501], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.553173392330243e-13\n",
      "Iteration 610/1000: Loss = 4513.674764955169\n",
      "gradient in iteration610: [5179416.62242265  -13268.97076979], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307193703453312e-12\n",
      "Iteration 611/1000: Loss = 4513.30363322898\n",
      "gradient in iteration611: [-39148049.58562854     89112.71033867], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5544056743176945e-13\n",
      "Iteration 612/1000: Loss = 4513.674752144045\n",
      "gradient in iteration612: [5179482.38425561  -13269.08800365], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9306948567674676e-12\n",
      "Iteration 613/1000: Loss = 4513.303595337636\n",
      "gradient in iteration613: [-39209070.66694781    136672.75202351], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.550430252464446e-13\n",
      "Iteration 614/1000: Loss = 4513.674793710616\n",
      "gradient in iteration614: [5179269.71956513  -13268.82011199], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9307741325430804e-12\n",
      "Iteration 615/1000: Loss = 4513.303717927115\n",
      "gradient in iteration615: [-3.90092011e+07  1.44168250e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.563497768788015e-13\n",
      "Iteration 616/1000: Loss = 4513.6746597623005\n",
      "gradient in iteration616: [5179774.35529247  -13269.45739493], params: [0.08009705 0.04001708]\n",
      "learning_rate: 1.930586028285661e-12\n",
      "Iteration 617/1000: Loss = 4513.303427023826\n",
      "gradient in iteration617: [-39473409.35610555    347329.51430669], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.533350973002095e-13\n",
      "Iteration 618/1000: Loss = 4513.678314516902\n",
      "gradient in iteration618: [5178338.73527707  -13267.71994172], params: [0.08009705 0.04001701]\n",
      "learning_rate: 1.931121255524616e-12\n",
      "Iteration 619/1000: Loss = 4513.304254623438\n",
      "gradient in iteration619: [-3.80282390e+07  1.36989983e+04], params: [0.08008705 0.04001704]\n",
      "learning_rate: 2.6296247869068707e-13\n",
      "Iteration 620/1000: Loss = 4513.674091180573\n",
      "gradient in iteration620: [5178845.51022689  -13268.32302766], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.930932286018682e-12\n",
      "Iteration 621/1000: Loss = 4513.303962471219\n",
      "gradient in iteration621: [-3.85858115e+07  1.40891019e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.5916261980202145e-13\n",
      "Iteration 622/1000: Loss = 4513.674397094558\n",
      "gradient in iteration622: [5179351.1685446   -13268.92393081], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307437697471297e-12\n",
      "Iteration 623/1000: Loss = 4513.303670975161\n",
      "gradient in iteration623: [-39086662.14420009     41715.68748461], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.558417488581552e-13\n",
      "Iteration 624/1000: Loss = 4513.674710884866\n",
      "gradient in iteration624: [5179695.17641177  -13269.30082541], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930615539991582e-12\n",
      "Iteration 625/1000: Loss = 4513.30347265134\n",
      "gradient in iteration625: [-39402768.44132313    290501.47395537], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.537892740935592e-13\n",
      "Iteration 626/1000: Loss = 4513.677120017615\n",
      "gradient in iteration626: [5178587.88734884  -13267.99256276], params: [0.08009705 0.04001702]\n",
      "learning_rate: 1.9310283454742073e-12\n",
      "Iteration 627/1000: Loss = 4513.3041109831565\n",
      "gradient in iteration627: [-3.83107541e+07  1.38905801e+04], params: [0.08008705 0.04001705]\n",
      "learning_rate: 2.610233143108573e-13\n",
      "Iteration 628/1000: Loss = 4513.674240515625\n",
      "gradient in iteration628: [5179094.12778863  -13268.6035613 ], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.930839593423223e-12\n",
      "Iteration 629/1000: Loss = 4513.303819145044\n",
      "gradient in iteration629: [-3.88378655e+07  1.42810924e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.5748067945157815e-13\n",
      "Iteration 630/1000: Loss = 4513.674550310805\n",
      "gradient in iteration630: [5179599.21066904  -13269.25125795], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930651309738755e-12\n",
      "Iteration 631/1000: Loss = 4513.303527993125\n",
      "gradient in iteration631: [-39316099.75657011    221142.47872196], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5434872894097035e-13\n",
      "Iteration 632/1000: Loss = 4513.675670692858\n",
      "gradient in iteration632: [5178894.2083931   -13268.36372755], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.930914129080613e-12\n",
      "Iteration 633/1000: Loss = 4513.303934402778\n",
      "gradient in iteration633: [-3.86361271e+07  1.41266154e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.588251138189585e-13\n",
      "Iteration 634/1000: Loss = 4513.6744269355\n",
      "gradient in iteration634: [5179399.73296961  -13268.96567662], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307256662089087e-12\n",
      "Iteration 635/1000: Loss = 4513.303642973831\n",
      "gradient in iteration635: [-39132261.13322707     76879.29091844], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.555436284643678e-13\n",
      "Iteration 636/1000: Loss = 4513.674741478875\n",
      "gradient in iteration636: [5179537.20514015  -13269.10855823], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9306744220460557e-12\n",
      "Iteration 637/1000: Loss = 4513.303563721099\n",
      "gradient in iteration637: [-39259536.02506524    176336.31291574], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.547151854676913e-13\n",
      "Iteration 638/1000: Loss = 4513.67482849925\n",
      "gradient in iteration638: [5179093.04071509  -13268.60456174], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9308399986997107e-12\n",
      "Iteration 639/1000: Loss = 4513.303819773523\n",
      "gradient in iteration639: [-3.88367869e+07  1.42801753e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.5748783061790714e-13\n",
      "Iteration 640/1000: Loss = 4513.6745496359945\n",
      "gradient in iteration640: [5179598.09258173  -13269.23706984], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306517264963284e-12\n",
      "Iteration 641/1000: Loss = 4513.30352861958\n",
      "gradient in iteration641: [-39315113.11066061    220357.06138013], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.543551120367608e-13\n",
      "Iteration 642/1000: Loss = 4513.675654287013\n",
      "gradient in iteration642: [5178897.68329951  -13268.35704276], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9309128334871703e-12\n",
      "Iteration 643/1000: Loss = 4513.3039323976955\n",
      "gradient in iteration643: [-3.86397033e+07  1.41292365e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.588011590449092e-13\n",
      "Iteration 644/1000: Loss = 4513.674429070944\n",
      "gradient in iteration644: [5179403.20778507  -13268.98182015], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930724370902264e-12\n",
      "Iteration 645/1000: Loss = 4513.303640972453\n",
      "gradient in iteration645: [-39135507.04864974     79391.97544147], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5552243356829135e-13\n",
      "Iteration 646/1000: Loss = 4513.674743667091\n",
      "gradient in iteration646: [5179525.94118469  -13269.11592514], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9306786206987776e-12\n",
      "Iteration 647/1000: Loss = 4513.303570217528\n",
      "gradient in iteration647: [-39249199.08354241    168187.57990222], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.547822690270667e-13\n",
      "Iteration 648/1000: Loss = 4513.674821342061\n",
      "gradient in iteration648: [5179129.27598445  -13268.63430222], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.930826489767278e-12\n",
      "Iteration 649/1000: Loss = 4513.30379887758\n",
      "gradient in iteration649: [-3.88725847e+07  1.43081043e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.572507094443794e-13\n",
      "Iteration 650/1000: Loss = 4513.674572142165\n",
      "gradient in iteration650: [5179634.27391805  -13269.23734268], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306382403010195e-12\n",
      "Iteration 651/1000: Loss = 4513.303507774716\n",
      "gradient in iteration651: [-39347891.51817313    246485.79596942], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5414322379590337e-13\n",
      "Iteration 652/1000: Loss = 4513.6762002348905\n",
      "gradient in iteration652: [5178782.07260765  -13268.20370195], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.9309559390215375e-12\n",
      "Iteration 653/1000: Loss = 4513.303999044885\n",
      "gradient in iteration653: [-3.85194917e+07  1.40401550e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.5960882523524783e-13\n",
      "Iteration 654/1000: Loss = 4513.674358326921\n",
      "gradient in iteration654: [5179287.87685378  -13268.87131654], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9307673637315996e-12\n",
      "Iteration 655/1000: Loss = 4513.3037074637305\n",
      "gradient in iteration655: [-3.90266317e+07  1.44308255e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5623528251776993e-13\n",
      "Iteration 656/1000: Loss = 4513.674671136897\n",
      "gradient in iteration656: [5179792.44982598  -13269.44461653], params: [0.08009705 0.04001708]\n",
      "learning_rate: 1.9305792841827004e-12\n",
      "Iteration 657/1000: Loss = 4513.30341658525\n",
      "gradient in iteration657: [-39489468.66921688    347299.83826765], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.5323207267651273e-13\n",
      "Iteration 658/1000: Loss = 4513.678587706743\n",
      "gradient in iteration658: [5178357.82075053  -13267.71794083], params: [0.08009705 0.04001701]\n",
      "learning_rate: 1.931114138140155e-12\n",
      "Iteration 659/1000: Loss = 4513.304243612642\n",
      "gradient in iteration659: [-3.80505496e+07  1.37137746e+04], params: [0.08008705 0.04001704]\n",
      "learning_rate: 2.6280829313728307e-13\n",
      "Iteration 660/1000: Loss = 4513.674102553593\n",
      "gradient in iteration660: [5178864.58524117  -13268.30310973], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9309251739267705e-12\n",
      "Iteration 661/1000: Loss = 4513.303951485646\n",
      "gradient in iteration661: [-3.86055622e+07  1.41038153e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.590300314828596e-13\n",
      "Iteration 662/1000: Loss = 4513.674408764763\n",
      "gradient in iteration662: [5179370.16506949  -13268.93261648], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930736688302687e-12\n",
      "Iteration 663/1000: Loss = 4513.3036600162895\n",
      "gradient in iteration663: [-39104548.06441521     55478.60391744], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.557247301139356e-13\n",
      "Iteration 664/1000: Loss = 4513.674722848617\n",
      "gradient in iteration664: [5179633.33300031  -13269.2560328 ], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930638591015377e-12\n",
      "Iteration 665/1000: Loss = 4513.3035083295745\n",
      "gradient in iteration665: [-39347021.69167394    245791.32582403], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5414884202318315e-13\n",
      "Iteration 666/1000: Loss = 4513.676185717301\n",
      "gradient in iteration666: [5178785.17584904  -13268.2218009 ], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.9309547819504868e-12\n",
      "Iteration 667/1000: Loss = 4513.303997275344\n",
      "gradient in iteration667: [-3.85227206e+07  1.40426041e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.59587065620882e-13\n",
      "Iteration 668/1000: Loss = 4513.674360201067\n",
      "gradient in iteration668: [5179290.90565303  -13268.83343609], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.930766234637509e-12\n",
      "Iteration 669/1000: Loss = 4513.30370569864\n",
      "gradient in iteration669: [-3.90295651e+07  1.44332037e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5621602432177407e-13\n",
      "Iteration 670/1000: Loss = 4513.67467305438\n",
      "gradient in iteration670: [5179795.50704695  -13269.4407057 ], params: [0.08009705 0.04001708]\n",
      "learning_rate: 1.930578144715427e-12\n",
      "Iteration 671/1000: Loss = 4513.30341482582\n",
      "gradient in iteration671: [-39492172.00406565    347295.14641185], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.5321473832764933e-13\n",
      "Iteration 672/1000: Loss = 4513.67863374426\n",
      "gradient in iteration672: [5178361.06882896  -13267.72971879], params: [0.08009705 0.04001701]\n",
      "learning_rate: 1.9311129268669183e-12\n",
      "Iteration 673/1000: Loss = 4513.304241758176\n",
      "gradient in iteration673: [-3.80542978e+07  1.37161674e+04], params: [0.08008705 0.04001704]\n",
      "learning_rate: 2.627824080481119e-13\n",
      "Iteration 674/1000: Loss = 4513.674104472087\n",
      "gradient in iteration674: [5178867.80176013  -13268.33166786], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9309239746574175e-12\n",
      "Iteration 675/1000: Loss = 4513.303949634582\n",
      "gradient in iteration675: [-3.86088842e+07  1.41062216e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.590077441664798e-13\n",
      "Iteration 676/1000: Loss = 4513.674410731546\n",
      "gradient in iteration676: [5179373.3935483   -13268.97040599], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307354848091315e-12\n",
      "Iteration 677/1000: Loss = 4513.303658168711\n",
      "gradient in iteration677: [-39107559.9429314      57799.97709396], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5570503540984736e-13\n",
      "Iteration 678/1000: Loss = 4513.6747248680795\n",
      "gradient in iteration678: [5179622.86080547  -13269.24566456], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930642494392909e-12\n",
      "Iteration 679/1000: Loss = 4513.303514343414\n",
      "gradient in iteration679: [-39337580.32808328    238253.58457543], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5420983996976945e-13\n",
      "Iteration 680/1000: Loss = 4513.676028216897\n",
      "gradient in iteration680: [5178818.49013975  -13268.29697064], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.9309423605093672e-12\n",
      "Iteration 681/1000: Loss = 4513.303978063044\n",
      "gradient in iteration681: [-3.85576469e+07  1.40681429e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.5935192614144674e-13\n",
      "Iteration 682/1000: Loss = 4513.674380552377\n",
      "gradient in iteration682: [5179324.19134013  -13268.86426796], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307538262849195e-12\n",
      "Iteration 683/1000: Loss = 4513.303686529456\n",
      "gradient in iteration683: [-3.90611824e+07  2.21764876e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.560086354978069e-13\n",
      "Iteration 684/1000: Loss = 4513.674693922257\n",
      "gradient in iteration684: [5179783.18825953  -13269.41469415], params: [0.08009705 0.04001708]\n",
      "learning_rate: 1.93058273610099e-12\n",
      "Iteration 685/1000: Loss = 4513.303421921915\n",
      "gradient in iteration685: [-39481262.01698869    347315.02141767], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.532847099896914e-13\n",
      "Iteration 686/1000: Loss = 4513.678448015514\n",
      "gradient in iteration686: [5178348.05791608  -13267.71089225], params: [0.08009705 0.04001701]\n",
      "learning_rate: 1.9311177789050145e-12\n",
      "Iteration 687/1000: Loss = 4513.304249242802\n",
      "gradient in iteration687: [-3.80391562e+07  1.37062047e+04], params: [0.08008705 0.04001704]\n",
      "learning_rate: 2.6288700907819225e-13\n",
      "Iteration 688/1000: Loss = 4513.674096736743\n",
      "gradient in iteration688: [5178854.81131088  -13268.28823949], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9309288181162153e-12\n",
      "Iteration 689/1000: Loss = 4513.303957103248\n",
      "gradient in iteration689: [-3.85954712e+07  1.40962561e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.59097756292231e-13\n",
      "Iteration 690/1000: Loss = 4513.674402795293\n",
      "gradient in iteration690: [5179360.47076546  -13268.91560893], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307403020979725e-12\n",
      "Iteration 691/1000: Loss = 4513.303665620527\n",
      "gradient in iteration691: [-39095407.60087111     48440.73100685], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.557845182761871e-13\n",
      "Iteration 692/1000: Loss = 4513.674716728784\n",
      "gradient in iteration692: [5179664.94467146  -13269.269175  ], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306268082624583e-12\n",
      "Iteration 693/1000: Loss = 4513.303490091134\n",
      "gradient in iteration693: [-39375576.57441175    268649.55275414], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.539645351250173e-13\n",
      "Iteration 694/1000: Loss = 4513.67666335802\n",
      "gradient in iteration694: [5178684.22334686  -13268.11552644], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.930992423696619e-12\n",
      "Iteration 695/1000: Loss = 4513.304055462691\n",
      "gradient in iteration695: [-3.84154204e+07  1.39648227e+04], params: [0.08008705 0.04001705]\n",
      "learning_rate: 2.6031213259586905e-13\n",
      "Iteration 696/1000: Loss = 4513.6742987926755\n",
      "gradient in iteration696: [5179190.21704074  -13268.72520622], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9308037706546623e-12\n",
      "Iteration 697/1000: Loss = 4513.303763750898\n",
      "gradient in iteration697: [-3.89322616e+07  1.43553802e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5685638563266073e-13\n",
      "Iteration 698/1000: Loss = 4513.674610082487\n",
      "gradient in iteration698: [5179695.04849134  -13269.34730059], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306155876710628e-12\n",
      "Iteration 699/1000: Loss = 4513.303472734473\n",
      "gradient in iteration699: [-39402640.77232314    290398.49669061], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.5379009639943e-13\n",
      "Iteration 700/1000: Loss = 4513.677117872713\n",
      "gradient in iteration700: [5178588.34177787  -13267.98201262], params: [0.08009705 0.04001702]\n",
      "learning_rate: 1.931028176023524e-12\n",
      "Iteration 701/1000: Loss = 4513.304110721778\n",
      "gradient in iteration701: [-3.83112518e+07  1.38908697e+04], params: [0.08008705 0.04001705]\n",
      "learning_rate: 2.61019923458327e-13\n",
      "Iteration 702/1000: Loss = 4513.674240787758\n",
      "gradient in iteration702: [5179094.59899784  -13268.60242443], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.930839417749778e-12\n",
      "Iteration 703/1000: Loss = 4513.303818884388\n",
      "gradient in iteration703: [-3.88383137e+07  1.42814412e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.574777080064717e-13\n",
      "Iteration 704/1000: Loss = 4513.674550592114\n",
      "gradient in iteration704: [5179599.65595764  -13269.21742475], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306511437612494e-12\n",
      "Iteration 705/1000: Loss = 4513.303527733003\n",
      "gradient in iteration705: [-39316508.28008059    221467.26105348], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5434608609601337e-13\n",
      "Iteration 706/1000: Loss = 4513.675677482943\n",
      "gradient in iteration706: [5178892.75547532  -13268.34749307], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9309146707909005e-12\n",
      "Iteration 707/1000: Loss = 4513.3039352319765\n",
      "gradient in iteration707: [-3.86346470e+07  1.41255746e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.588350296450347e-13\n",
      "Iteration 708/1000: Loss = 4513.67442605286\n",
      "gradient in iteration708: [5179398.30951946  -13268.99641754], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930726196828797e-12\n",
      "Iteration 709/1000: Loss = 4513.303643800977\n",
      "gradient in iteration709: [-39130919.54814263     75841.07202092], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5555238965690644e-13\n",
      "Iteration 710/1000: Loss = 4513.674740574089\n",
      "gradient in iteration710: [5179541.88235308  -13269.15312347], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306726786147694e-12\n",
      "Iteration 711/1000: Loss = 4513.303561036523\n",
      "gradient in iteration711: [-39263805.81024205    179705.2877464 ], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.54687486188399e-13\n",
      "Iteration 712/1000: Loss = 4513.67483146008\n",
      "gradient in iteration712: [5179078.08689415  -13268.59405708], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.930845573714243e-12\n",
      "Iteration 713/1000: Loss = 4513.303828407332\n",
      "gradient in iteration713: [-3.88219268e+07  1.42685764e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.575863905382879e-13\n",
      "Iteration 714/1000: Loss = 4513.674540346678\n",
      "gradient in iteration714: [5179583.17068405  -13269.17463302], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306572885244986e-12\n",
      "Iteration 715/1000: Loss = 4513.303537232774\n",
      "gradient in iteration715: [-39301517.53420432    209555.92067367], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5444310111681944e-13\n",
      "Iteration 716/1000: Loss = 4513.675428621375\n",
      "gradient in iteration716: [5178945.55305247  -13268.46404481], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.930894985776786e-12\n",
      "Iteration 717/1000: Loss = 4513.303904802492\n",
      "gradient in iteration717: [-3.86886685e+07  1.41663496e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.584736145259685e-13\n",
      "Iteration 718/1000: Loss = 4513.674458495503\n",
      "gradient in iteration718: [5179450.97012671  -13269.04584857], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307065667146112e-12\n",
      "Iteration 719/1000: Loss = 4513.303613443288\n",
      "gradient in iteration719: [-39179987.62344723    113950.59918868], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.552323419830666e-13\n",
      "Iteration 720/1000: Loss = 4513.674773830556\n",
      "gradient in iteration720: [5179371.20512215  -13268.90428572], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.93073630059774e-12\n",
      "Iteration 721/1000: Loss = 4513.303659423147\n",
      "gradient in iteration721: [-39105513.32992327     56222.72292385], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.557184179026764e-13\n",
      "Iteration 722/1000: Loss = 4513.674723495574\n",
      "gradient in iteration722: [5179629.9446778   -13269.22988483], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306398539678023e-12\n",
      "Iteration 723/1000: Loss = 4513.303510257493\n",
      "gradient in iteration723: [-39343995.45701126    243374.00577679], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5416839047082497e-13\n",
      "Iteration 724/1000: Loss = 4513.676135207604\n",
      "gradient in iteration724: [5178795.82321257  -13268.24326497], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.930950811997197e-12\n",
      "Iteration 725/1000: Loss = 4513.303991115654\n",
      "gradient in iteration725: [-3.85339454e+07  1.40507665e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.5951144885236324e-13\n",
      "Iteration 726/1000: Loss = 4513.674366722861\n",
      "gradient in iteration726: [5179301.58507625  -13268.8442136 ], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.930762253508122e-12\n",
      "Iteration 727/1000: Loss = 4513.30369955267\n",
      "gradient in iteration727: [-3.90397658e+07  1.44414890e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.561490773981232e-13\n",
      "Iteration 728/1000: Loss = 4513.674679742964\n",
      "gradient in iteration728: [5179806.15863962  -13269.4664444 ], params: [0.08009705 0.04001708]\n",
      "learning_rate: 1.930574174734429e-12\n",
      "Iteration 729/1000: Loss = 4513.30340869505\n",
      "gradient in iteration729: [-39501584.85185966    347277.90148281], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.531543996906043e-13\n",
      "Iteration 730/1000: Loss = 4513.678794197723\n",
      "gradient in iteration730: [5178372.26016127  -13267.76637143], params: [0.08009705 0.04001701]\n",
      "learning_rate: 1.9311087534074994e-12\n",
      "Iteration 731/1000: Loss = 4513.304235292682\n",
      "gradient in iteration731: [-3.80673364e+07  1.37248279e+04], params: [0.08008705 0.04001704]\n",
      "learning_rate: 2.6269240109577e-13\n",
      "Iteration 732/1000: Loss = 4513.674111156835\n",
      "gradient in iteration732: [5178879.003779    -13268.35690634], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9309197980302397e-12\n",
      "Iteration 733/1000: Loss = 4513.3039431818115\n",
      "gradient in iteration733: [-3.86204431e+07  1.41149167e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.5893022426667946e-13\n",
      "Iteration 734/1000: Loss = 4513.674417594799\n",
      "gradient in iteration734: [5179384.52494491  -13268.94780505], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307313353233548e-12\n",
      "Iteration 735/1000: Loss = 4513.303651731117\n",
      "gradient in iteration735: [-39118035.98052756     65882.56669602], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5563655611385673e-13\n",
      "Iteration 736/1000: Loss = 4513.6747319007845\n",
      "gradient in iteration736: [5179586.59456781  -13269.19255007], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306560122940497e-12\n",
      "Iteration 737/1000: Loss = 4513.303535271501\n",
      "gradient in iteration737: [-39304615.77124333    212015.63504292], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5442304431115595e-13\n",
      "Iteration 738/1000: Loss = 4513.675480008304\n",
      "gradient in iteration738: [5178934.64029839  -13268.41434093], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.930899054447969e-12\n",
      "Iteration 739/1000: Loss = 4513.303911089371\n",
      "gradient in iteration739: [-3.86775527e+07  1.41578622e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.5854789931796327e-13\n",
      "Iteration 740/1000: Loss = 4513.6744517848365\n",
      "gradient in iteration740: [5179440.09407074  -13269.02865912], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930710620912033e-12\n",
      "Iteration 741/1000: Loss = 4513.303619714972\n",
      "gradient in iteration741: [-39169881.71595222    106078.07203087], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5529819243562914e-13\n",
      "Iteration 742/1000: Loss = 4513.674766953186\n",
      "gradient in iteration742: [5179406.4220303   -13268.96986029], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930723172729909e-12\n",
      "Iteration 743/1000: Loss = 4513.30363912628\n",
      "gradient in iteration743: [-39138499.57489219     81709.55361493], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5550289634544697e-13\n",
      "Iteration 744/1000: Loss = 4513.674745688551\n",
      "gradient in iteration744: [5179515.55789295  -13269.10387434], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9306824910992323e-12\n",
      "Iteration 745/1000: Loss = 4513.303576208531\n",
      "gradient in iteration745: [-39239651.6842382     160671.95278993], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.548442600986901e-13\n",
      "Iteration 746/1000: Loss = 4513.674814746785\n",
      "gradient in iteration746: [5179162.7583043   -13268.69137301], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9308140073346685e-12\n",
      "Iteration 747/1000: Loss = 4513.303779592543\n",
      "gradient in iteration747: [-3.89054254e+07  1.43341168e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.5703356048439986e-13\n",
      "Iteration 748/1000: Loss = 4513.674592958952\n",
      "gradient in iteration748: [5179667.66901736  -13269.27904302], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306257928121295e-12\n",
      "Iteration 749/1000: Loss = 4513.303488537439\n",
      "gradient in iteration749: [-39378003.29763377    270596.11311415], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.5394888421376365e-13\n",
      "Iteration 750/1000: Loss = 4513.6767040565255\n",
      "gradient in iteration750: [5178675.62498395  -13268.14640379], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.930995629800812e-12\n",
      "Iteration 751/1000: Loss = 4513.304060412409\n",
      "gradient in iteration751: [-3.84061844e+07  1.39581330e+04], params: [0.08008705 0.04001705]\n",
      "learning_rate: 2.60374732781887e-13\n",
      "Iteration 752/1000: Loss = 4513.674293584997\n",
      "gradient in iteration752: [5179181.68693541  -13268.72566096], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.930806950685898e-12\n",
      "Iteration 753/1000: Loss = 4513.303768688054\n",
      "gradient in iteration753: [-3.89239121e+07  1.43486373e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5691148325926347e-13\n",
      "Iteration 754/1000: Loss = 4513.674604743879\n",
      "gradient in iteration754: [5179686.51997762  -13269.27913397], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306187664892132e-12\n",
      "Iteration 755/1000: Loss = 4513.303477659024\n",
      "gradient in iteration755: [-39394971.74078497    284227.48864614], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.5383950179731097e-13\n",
      "Iteration 756/1000: Loss = 4513.67698889991\n",
      "gradient in iteration756: [5178615.50997622  -13268.0592742 ], params: [0.08009705 0.04001702]\n",
      "learning_rate: 1.9310180454091893e-12\n",
      "Iteration 757/1000: Loss = 4513.304095053173\n",
      "gradient in iteration757: [-3.83410253e+07  1.39119142e+04], params: [0.08008705 0.04001705]\n",
      "learning_rate: 2.608172299126925e-13\n",
      "Iteration 758/1000: Loss = 4513.674257205466\n",
      "gradient in iteration758: [5179121.69839292  -13268.64248767], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.93082931476644e-12\n",
      "Iteration 759/1000: Loss = 4513.3038032506465\n",
      "gradient in iteration759: [-3.88651125e+07  1.43024069e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.5730016857404224e-13\n",
      "Iteration 760/1000: Loss = 4513.674567430653\n",
      "gradient in iteration760: [5179626.69446206  -13269.25766989], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306410654443067e-12\n",
      "Iteration 761/1000: Loss = 4513.303512137252\n",
      "gradient in iteration761: [-39341045.40762746    241019.27720039], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5418744968229024e-13\n",
      "Iteration 762/1000: Loss = 4513.676085996947\n",
      "gradient in iteration762: [5178806.23078781  -13268.2696858 ], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.9309469314666317e-12\n",
      "Iteration 763/1000: Loss = 4513.303985114233\n",
      "gradient in iteration763: [-3.85448570e+07  1.40588142e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.594379840543726e-13\n",
      "Iteration 764/1000: Loss = 4513.67437307811\n",
      "gradient in iteration764: [5179311.97096004  -13268.85803792], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307583818215897e-12\n",
      "Iteration 765/1000: Loss = 4513.303693564313\n",
      "gradient in iteration765: [-3.90496223e+07  1.44495736e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5608442325829653e-13\n",
      "Iteration 766/1000: Loss = 4513.674686259652\n",
      "gradient in iteration766: [5179816.4957745   -13269.47140114], params: [0.08009705 0.04001708]\n",
      "learning_rate: 1.930570321971371e-12\n",
      "Iteration 767/1000: Loss = 4513.303402722421\n",
      "gradient in iteration767: [-39510742.64500675    347261.0985682 ], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.5309572360730533e-13\n",
      "Iteration 768/1000: Loss = 4513.678950489097\n",
      "gradient in iteration768: [5178383.23526121  -13267.7757847 ], params: [0.08009705 0.04001701]\n",
      "learning_rate: 1.9311046606027373e-12\n",
      "Iteration 769/1000: Loss = 4513.304228994005\n",
      "gradient in iteration769: [-3.80799980e+07  1.37331398e+04], params: [0.08008705 0.04001704]\n",
      "learning_rate: 2.626050559316241e-13\n",
      "Iteration 770/1000: Loss = 4513.674117673974\n",
      "gradient in iteration770: [5178889.88956656  -13268.3769607 ], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9309157393259297e-12\n",
      "Iteration 771/1000: Loss = 4513.303936896449\n",
      "gradient in iteration771: [-3.86316770e+07  1.41233487e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.588549286277281e-13\n",
      "Iteration 772/1000: Loss = 4513.674424281655\n",
      "gradient in iteration772: [5179395.42810382  -13268.94821432], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307272709357537e-12\n",
      "Iteration 773/1000: Loss = 4513.303645460391\n",
      "gradient in iteration773: [-39128223.45496352     73755.4179068 ], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.555699982522839e-13\n",
      "Iteration 774/1000: Loss = 4513.674738756843\n",
      "gradient in iteration774: [5179551.21226805  -13269.16212747], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306692008980336e-12\n",
      "Iteration 775/1000: Loss = 4513.303555642137\n",
      "gradient in iteration775: [-39272370.67505302    186470.43757483], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5463194169615785e-13\n",
      "Iteration 776/1000: Loss = 4513.6749463256065\n",
      "gradient in iteration776: [5179048.00453834  -13268.54558101], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.930856788976875e-12\n",
      "Iteration 777/1000: Loss = 4513.303845738151\n",
      "gradient in iteration777: [-3.87919803e+07  1.42454009e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.5778524106536713e-13\n",
      "Iteration 778/1000: Loss = 4513.674521725991\n",
      "gradient in iteration778: [5179553.19187422  -13269.14230049], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306684630033703e-12\n",
      "Iteration 779/1000: Loss = 4513.303554521674\n",
      "gradient in iteration779: [-39274147.22120374    187875.12140079], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.546204235492883e-13\n",
      "Iteration 780/1000: Loss = 4513.674975670374\n",
      "gradient in iteration780: [5179041.7671782   -13268.53175669], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9308591143972353e-12\n",
      "Iteration 781/1000: Loss = 4513.3038493361655\n",
      "gradient in iteration781: [-3.87857437e+07  1.42405284e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.5782669200185827e-13\n",
      "Iteration 782/1000: Loss = 4513.674517865042\n",
      "gradient in iteration782: [5179546.94769287  -13269.13534285], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930670790512732e-12\n",
      "Iteration 783/1000: Loss = 4513.303558110229\n",
      "gradient in iteration783: [-39268452.17865776    183374.60610383], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.546573507533093e-13\n",
      "Iteration 784/1000: Loss = 4513.674881631943\n",
      "gradient in iteration784: [5179061.73595293  -13268.5740482 ], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9308516696335613e-12\n",
      "Iteration 785/1000: Loss = 4513.303837809377\n",
      "gradient in iteration785: [-3.88057020e+07  1.42558851e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.576940885985956e-13\n",
      "Iteration 786/1000: Loss = 4513.674530241132\n",
      "gradient in iteration786: [5179566.89595849  -13269.14871242], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306633548459044e-12\n",
      "Iteration 787/1000: Loss = 4513.303546610906\n",
      "gradient in iteration787: [-39286685.04354391    197795.77701229], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.54539164831962e-13\n",
      "Iteration 788/1000: Loss = 4513.675182923675\n",
      "gradient in iteration788: [5178997.7239423   -13268.51515842], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9308755348106054e-12\n",
      "Iteration 789/1000: Loss = 4513.303874729361\n",
      "gradient in iteration789: [-3.87415188e+07  1.42064646e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.581210110875036e-13\n",
      "Iteration 790/1000: Loss = 4513.674490646034\n",
      "gradient in iteration790: [5179503.01268684  -13269.10878561], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930687167379899e-12\n",
      "Iteration 791/1000: Loss = 4513.303583440987\n",
      "gradient in iteration791: [-39228106.73366257    151599.11886258], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.549192615359834e-13\n",
      "Iteration 792/1000: Loss = 4513.674806789656\n",
      "gradient in iteration792: [5179203.16838172  -13268.75958512], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.930798942402674e-12\n",
      "Iteration 793/1000: Loss = 4513.303756295553\n",
      "gradient in iteration793: [-3.89448486e+07  1.43653026e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.567733695138284e-13\n",
      "Iteration 794/1000: Loss = 4513.674618149995\n",
      "gradient in iteration794: [5179707.94235225  -13269.3327032 ], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306107817844878e-12\n",
      "Iteration 795/1000: Loss = 4513.303465297361\n",
      "gradient in iteration795: [-39414203.97677575    299715.79642734], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.5371564032835363e-13\n",
      "Iteration 796/1000: Loss = 4513.6773125888285\n",
      "gradient in iteration796: [5178547.3180189   -13267.97969341], params: [0.08009705 0.04001702]\n",
      "learning_rate: 1.9310434733703643e-12\n",
      "Iteration 797/1000: Loss = 4513.304134364323\n",
      "gradient in iteration797: [-3.82659673e+07  1.38594085e+04], params: [0.08008705 0.04001705]\n",
      "learning_rate: 2.6132881807450887e-13\n",
      "Iteration 798/1000: Loss = 4513.674216064985\n",
      "gradient in iteration798: [5179053.6984754   -13268.54226136], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9308546661610754e-12\n",
      "Iteration 799/1000: Loss = 4513.303842473187\n",
      "gradient in iteration799: [-3.87976344e+07  1.42497846e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.5774767331843487e-13\n",
      "Iteration 800/1000: Loss = 4513.674525232168\n",
      "gradient in iteration800: [5179558.83215109  -13269.14748461], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930666360603335e-12\n",
      "Iteration 801/1000: Loss = 4513.30355126464\n",
      "gradient in iteration801: [-39279312.51623704    191959.64532628], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.545869405394064e-13\n",
      "Iteration 802/1000: Loss = 4513.675061001725\n",
      "gradient in iteration802: [5179023.62375935  -13268.52261627], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.930865878681048e-12\n",
      "Iteration 803/1000: Loss = 4513.303859793462\n",
      "gradient in iteration803: [-3.87675747e+07  1.42265185e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.5794752615780634e-13\n",
      "Iteration 804/1000: Loss = 4513.674506648151\n",
      "gradient in iteration804: [5179528.84024453  -13269.13466073], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9306775400690487e-12\n",
      "Iteration 805/1000: Loss = 4513.303568541974\n",
      "gradient in iteration805: [-39251867.93062494    170290.25166266], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5476494564983085e-13\n",
      "Iteration 806/1000: Loss = 4513.6748231900665\n",
      "gradient in iteration806: [5179119.92383233  -13268.63252871], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.930829976340926e-12\n",
      "Iteration 807/1000: Loss = 4513.303804271507\n",
      "gradient in iteration807: [-3.88633666e+07  1.43009529e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.573117273463344e-13\n",
      "Iteration 808/1000: Loss = 4513.674566330562\n",
      "gradient in iteration808: [5179624.95305255  -13269.2297484 ], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306417145331374e-12\n",
      "Iteration 809/1000: Loss = 4513.3035131553\n",
      "gradient in iteration809: [-39339446.56810126    239742.37205948], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.541977804057007e-13\n",
      "Iteration 810/1000: Loss = 4513.676059326497\n",
      "gradient in iteration810: [5178811.91417473  -13268.24999524], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.9309448123862887e-12\n",
      "Iteration 811/1000: Loss = 4513.303981858529\n",
      "gradient in iteration811: [-3.85507651e+07  1.40629983e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.5939822414993416e-13\n",
      "Iteration 812/1000: Loss = 4513.674376528678\n",
      "gradient in iteration812: [5179317.62615262  -13268.86799689], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307562736654067e-12\n",
      "Iteration 813/1000: Loss = 4513.303690317159\n",
      "gradient in iteration813: [-3.90549618e+07  1.74186895e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.560494116485708e-13\n",
      "Iteration 814/1000: Loss = 4513.6746897967405\n",
      "gradient in iteration814: [5179804.62714149  -13269.44702669], params: [0.08009705 0.04001708]\n",
      "learning_rate: 1.930574745541814e-12\n",
      "Iteration 815/1000: Loss = 4513.303409556305\n",
      "gradient in iteration815: [-39500263.24237115    347280.33169813], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.5316286979255367e-13\n",
      "Iteration 816/1000: Loss = 4513.6787716494\n",
      "gradient in iteration816: [5178370.67950495  -13267.77933173], params: [0.08009705 0.04001701]\n",
      "learning_rate: 1.9311093428630322e-12\n",
      "Iteration 817/1000: Loss = 4513.304236200925\n",
      "gradient in iteration817: [-3.80655087e+07  1.37236023e+04], params: [0.08008705 0.04001704]\n",
      "learning_rate: 2.6270501423887253e-13\n",
      "Iteration 818/1000: Loss = 4513.674110217\n",
      "gradient in iteration818: [5178877.40511469  -13268.35235886], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.930920394084622e-12\n",
      "Iteration 819/1000: Loss = 4513.303944087918\n",
      "gradient in iteration819: [-3.86188221e+07  1.41137124e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.589410930101156e-13\n",
      "Iteration 820/1000: Loss = 4513.674416631562\n",
      "gradient in iteration820: [5179382.96143257  -13268.94944214], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930731918157699e-12\n",
      "Iteration 821/1000: Loss = 4513.303652635005\n",
      "gradient in iteration821: [-39116566.03381743     64747.7620987 ], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5564616258376833e-13\n",
      "Iteration 822/1000: Loss = 4513.67473091409\n",
      "gradient in iteration822: [5179591.67050321  -13269.21865257], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930654120275174e-12\n",
      "Iteration 823/1000: Loss = 4513.303532333556\n",
      "gradient in iteration823: [-39309252.92088432    215699.29635916], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.543930310791324e-13\n",
      "Iteration 824/1000: Loss = 4513.675556965021\n",
      "gradient in iteration824: [5178918.31300403  -13268.40579168], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9309051418884996e-12\n",
      "Iteration 825/1000: Loss = 4513.303920501143\n",
      "gradient in iteration825: [-3.86608686e+07  1.41453025e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.586594753205564e-13\n",
      "Iteration 826/1000: Loss = 4513.67444174616\n",
      "gradient in iteration826: [5179423.80874956  -13269.01210632], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307166915182877e-12\n",
      "Iteration 827/1000: Loss = 4513.303629104154\n",
      "gradient in iteration827: [-39154722.27427017     94291.87362002], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.553970356360138e-13\n",
      "Iteration 828/1000: Loss = 4513.674756662679\n",
      "gradient in iteration828: [5179459.16681128  -13269.04871348], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930703511300479e-12\n",
      "Iteration 829/1000: Loss = 4513.303608712038\n",
      "gradient in iteration829: [-39187599.90178841    119888.24257969], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5518276253360513e-13\n",
      "Iteration 830/1000: Loss = 4513.674779022073\n",
      "gradient in iteration830: [5179344.65378858  -13268.92584075], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307461983023708e-12\n",
      "Iteration 831/1000: Loss = 4513.303674722722\n",
      "gradient in iteration831: [-3.90805336e+07  3.70088393e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.558818696917293e-13\n",
      "Iteration 832/1000: Loss = 4513.674706796684\n",
      "gradient in iteration832: [5179716.41170334  -13269.34598183], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306076250439975e-12\n",
      "Iteration 833/1000: Loss = 4513.303460439269\n",
      "gradient in iteration833: [-39421746.71979046    305800.97209167], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.536670957550395e-13\n",
      "Iteration 834/1000: Loss = 4513.677439771509\n",
      "gradient in iteration834: [5178520.55563708  -13267.93408225], params: [0.08009705 0.04001702]\n",
      "learning_rate: 1.931053452923828e-12\n",
      "Iteration 835/1000: Loss = 4513.304149794578\n",
      "gradient in iteration835: [-3.82361660e+07  1.38388399e+04], params: [0.08008705 0.04001705]\n",
      "learning_rate: 2.615324979730361e-13\n",
      "Iteration 836/1000: Loss = 4513.67419996168\n",
      "gradient in iteration836: [5179026.95519297  -13268.52793682], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.930864636642427e-12\n",
      "Iteration 837/1000: Loss = 4513.303857869872\n",
      "gradient in iteration837: [-3.87709212e+07  1.42291421e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.5792526146537687e-13\n",
      "Iteration 838/1000: Loss = 4513.674508710395\n",
      "gradient in iteration838: [5179532.19455194  -13269.14089077], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930676289746484e-12\n",
      "Iteration 839/1000: Loss = 4513.3035666232945\n",
      "gradient in iteration839: [-39254921.83711867    172697.01975238], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5474512575756043e-13\n",
      "Iteration 840/1000: Loss = 4513.674825303234\n",
      "gradient in iteration840: [5179109.22744704  -13268.63753093], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9308339640732652e-12\n",
      "Iteration 841/1000: Loss = 4513.303810443039\n",
      "gradient in iteration841: [-3.88527999e+07  1.42925453e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.5738170770743644e-13\n",
      "Iteration 842/1000: Loss = 4513.674559680216\n",
      "gradient in iteration842: [5179614.26935472  -13269.23165834], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.93064569675877e-12\n",
      "Iteration 843/1000: Loss = 4513.303519310949\n",
      "gradient in iteration843: [-39329770.08653324    232025.99759315], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5426032183758083e-13\n",
      "Iteration 844/1000: Loss = 4513.675898098921\n",
      "gradient in iteration844: [5178846.02763843  -13268.31534243], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.930932093101836e-12\n",
      "Iteration 845/1000: Loss = 4513.303962179985\n",
      "gradient in iteration845: [-3.85863360e+07  1.40894098e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.5915909740994333e-13\n",
      "Iteration 846/1000: Loss = 4513.674397402229\n",
      "gradient in iteration846: [5179351.66462848  -13268.93652731], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307435848184105e-12\n",
      "Iteration 847/1000: Loss = 4513.303670684267\n",
      "gradient in iteration847: [-39087137.8333486      42081.26133562], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5583863527270444e-13\n",
      "Iteration 848/1000: Loss = 4513.67471120282\n",
      "gradient in iteration848: [5179693.56410504  -13269.31501353], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306161409430467e-12\n",
      "Iteration 849/1000: Loss = 4513.303473599676\n",
      "gradient in iteration849: [-39401293.69239002    289313.62990079], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.537987731588469e-13\n",
      "Iteration 850/1000: Loss = 4513.677095200994\n",
      "gradient in iteration850: [5178593.13772533  -13267.98951595], params: [0.08009705 0.04001702]\n",
      "learning_rate: 1.931026387678807e-12\n",
      "Iteration 851/1000: Loss = 4513.304107967512\n",
      "gradient in iteration851: [-3.83164985e+07  1.38946314e+04], params: [0.08008705 0.04001705]\n",
      "learning_rate: 2.6098418122004015e-13\n",
      "Iteration 852/1000: Loss = 4513.6742436711265\n",
      "gradient in iteration852: [5179099.3553368   -13268.64817202], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9308376445212433e-12\n",
      "Iteration 853/1000: Loss = 4513.303816137129\n",
      "gradient in iteration853: [-3.88430343e+07  1.42851134e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.57446416557966e-13\n",
      "Iteration 854/1000: Loss = 4513.674553549781\n",
      "gradient in iteration854: [5179604.40865863  -13269.21333202], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306493722345313e-12\n",
      "Iteration 855/1000: Loss = 4513.303524991021\n",
      "gradient in iteration855: [-39320828.22429948    224904.99168271], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5431814261277954e-13\n",
      "Iteration 856/1000: Loss = 4513.675749308958\n",
      "gradient in iteration856: [5178877.54399453  -13268.35290456], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9309203423039195e-12\n",
      "Iteration 857/1000: Loss = 4513.303944009102\n",
      "gradient in iteration857: [-3.86189628e+07  1.41137306e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.589401491410803e-13\n",
      "Iteration 858/1000: Loss = 4513.674416715275\n",
      "gradient in iteration858: [5179383.11054422  -13268.95189777], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307318625729644e-12\n",
      "Iteration 859/1000: Loss = 4513.303652556289\n",
      "gradient in iteration859: [-39116694.63805083     64846.99688372], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5564532209407296e-13\n",
      "Iteration 860/1000: Loss = 4513.67473100052\n",
      "gradient in iteration860: [5179591.22298635  -13269.21237705], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306542870837594e-12\n",
      "Iteration 861/1000: Loss = 4513.303532591202\n",
      "gradient in iteration861: [-39308847.52221572    215376.79717767], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.543956546766836e-13\n",
      "Iteration 862/1000: Loss = 4513.6755502318565\n",
      "gradient in iteration862: [5178919.76114697  -13268.42689195], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9309046019638096e-12\n",
      "Iteration 863/1000: Loss = 4513.303919677445\n",
      "gradient in iteration863: [-3.86623320e+07  1.41464316e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.586496854075982e-13\n",
      "Iteration 864/1000: Loss = 4513.674442626156\n",
      "gradient in iteration864: [5179425.23215424  -13268.99587184], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930716160920575e-12\n",
      "Iteration 865/1000: Loss = 4513.303628281937\n",
      "gradient in iteration865: [-39156051.18613712     95323.59731566], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.553883677509447e-13\n",
      "Iteration 866/1000: Loss = 4513.674757562964\n",
      "gradient in iteration866: [5179454.54530491  -13268.99891865], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307052340221507e-12\n",
      "Iteration 867/1000: Loss = 4513.303611375488\n",
      "gradient in iteration867: [-39183313.67790415    116543.61478577], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5521067672331904e-13\n",
      "Iteration 868/1000: Loss = 4513.674776096969\n",
      "gradient in iteration868: [5179359.59614988  -13268.94707745], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.93074062813356e-12\n",
      "Iteration 869/1000: Loss = 4513.303666105311\n",
      "gradient in iteration869: [-39094617.04611203     47832.31361216], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5578969064219296e-13\n",
      "Iteration 870/1000: Loss = 4513.674716201411\n",
      "gradient in iteration870: [5179667.71094507  -13269.28022536], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930625777184349e-12\n",
      "Iteration 871/1000: Loss = 4513.30348851345\n",
      "gradient in iteration871: [-39378040.97645312    270626.29401371], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.539486412231553e-13\n",
      "Iteration 872/1000: Loss = 4513.676704682191\n",
      "gradient in iteration872: [5178675.48478534  -13268.12543994], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.930995682077287e-12\n",
      "Iteration 873/1000: Loss = 4513.304060489337\n",
      "gradient in iteration873: [-3.84060399e+07  1.39580081e+04], params: [0.08008705 0.04001705]\n",
      "learning_rate: 2.603757121157844e-13\n",
      "Iteration 874/1000: Loss = 4513.674293504381\n",
      "gradient in iteration874: [5179181.51172125  -13268.70360572], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9308070160060085e-12\n",
      "Iteration 875/1000: Loss = 4513.303768765259\n",
      "gradient in iteration875: [-3.89237810e+07  1.43486395e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5691234886039786e-13\n",
      "Iteration 876/1000: Loss = 4513.67460466142\n",
      "gradient in iteration876: [5179686.38018829  -13269.32001575], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930618818592737e-12\n",
      "Iteration 877/1000: Loss = 4513.303477736315\n",
      "gradient in iteration877: [-39394852.0493268     284131.34391485], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.538402730254926e-13\n",
      "Iteration 878/1000: Loss = 4513.676986898468\n",
      "gradient in iteration878: [5178615.93261841  -13268.06532234], params: [0.08009705 0.04001702]\n",
      "learning_rate: 1.931017887813086e-12\n",
      "Iteration 879/1000: Loss = 4513.304094809394\n",
      "gradient in iteration879: [-3.83414875e+07  1.39122498e+04], params: [0.08008705 0.04001705]\n",
      "learning_rate: 2.6081408532275184e-13\n",
      "Iteration 880/1000: Loss = 4513.674257460736\n",
      "gradient in iteration880: [5179122.12103511  -13268.63962277], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9308291572011413e-12\n",
      "Iteration 881/1000: Loss = 4513.303803006909\n",
      "gradient in iteration881: [-3.88655293e+07  1.43027364e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.572974093395937e-13\n",
      "Iteration 882/1000: Loss = 4513.67456769353\n",
      "gradient in iteration882: [5179627.12692679  -13269.22806584], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930640904248501e-12\n",
      "Iteration 883/1000: Loss = 4513.30351189399\n",
      "gradient in iteration883: [-39341426.50955436    241322.72747011], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5418498735859067e-13\n",
      "Iteration 884/1000: Loss = 4513.676092348196\n",
      "gradient in iteration884: [5178804.88164337  -13268.25695287], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.9309474345028295e-12\n",
      "Iteration 885/1000: Loss = 4513.303985887309\n",
      "gradient in iteration885: [-3.85434525e+07  1.40578758e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.5944743781583023e-13\n",
      "Iteration 886/1000: Loss = 4513.674372260404\n",
      "gradient in iteration886: [5179310.63709511  -13268.87177129], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307588790636115e-12\n",
      "Iteration 887/1000: Loss = 4513.303694336548\n",
      "gradient in iteration887: [-3.90483534e+07  1.44485576e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.560927448138797e-13\n",
      "Iteration 888/1000: Loss = 4513.674685420069\n",
      "gradient in iteration888: [5179815.16231885  -13269.50523435], params: [0.08009705 0.04001708]\n",
      "learning_rate: 1.9305708189639924e-12\n",
      "Iteration 889/1000: Loss = 4513.3034034921675\n",
      "gradient in iteration889: [-39509564.38479806    347263.09249891], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.531032714662797e-13\n",
      "Iteration 890/1000: Loss = 4513.678930371552\n",
      "gradient in iteration890: [5178381.80071521  -13267.77251052], params: [0.08009705 0.04001701]\n",
      "learning_rate: 1.931105195568787e-12\n",
      "Iteration 891/1000: Loss = 4513.304229804316\n",
      "gradient in iteration891: [-3.80783709e+07  1.37321719e+04], params: [0.08008705 0.04001704]\n",
      "learning_rate: 2.6261627733832743e-13\n",
      "Iteration 892/1000: Loss = 4513.674116834701\n",
      "gradient in iteration892: [5178888.47830363  -13268.38983005], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9309162655063664e-12\n",
      "Iteration 893/1000: Loss = 4513.30393770549\n",
      "gradient in iteration893: [-3.86302339e+07  1.41223403e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.5886459888650534e-13\n",
      "Iteration 894/1000: Loss = 4513.674423423087\n",
      "gradient in iteration894: [5179394.02343472  -13268.99205196], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930727794555488e-12\n",
      "Iteration 895/1000: Loss = 4513.303646267524\n",
      "gradient in iteration895: [-39126915.49990086     72743.65671037], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.555785415802924e-13\n",
      "Iteration 896/1000: Loss = 4513.674737876896\n",
      "gradient in iteration896: [5179555.77333851  -13269.15548816], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306675007680155e-12\n",
      "Iteration 897/1000: Loss = 4513.30355302484\n",
      "gradient in iteration897: [-39276522.34774541    189752.69003931], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.546050261645436e-13\n",
      "Iteration 898/1000: Loss = 4513.675014888152\n",
      "gradient in iteration898: [5179033.45312334  -13268.53475803], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9308622140622136e-12\n",
      "Iteration 899/1000: Loss = 4513.303854143569\n",
      "gradient in iteration899: [-3.87773981e+07  1.42340337e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.5788218110399354e-13\n",
      "Iteration 900/1000: Loss = 4513.674512706629\n",
      "gradient in iteration900: [5179538.64086849  -13269.12979493], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930673886877932e-12\n",
      "Iteration 901/1000: Loss = 4513.303562905636\n",
      "gradient in iteration901: [-39260832.66469505    177359.17726895], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5470677317020867e-13\n",
      "Iteration 902/1000: Loss = 4513.674829398696\n",
      "gradient in iteration902: [5179088.48532898  -13268.5958306 ], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9308416970143353e-12\n",
      "Iteration 903/1000: Loss = 4513.303822394602\n",
      "gradient in iteration903: [-3.88322793e+07  1.42767210e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.5751771897411894e-13\n",
      "Iteration 904/1000: Loss = 4513.674546815603\n",
      "gradient in iteration904: [5179593.57980544  -13269.18663835], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306534085972876e-12\n",
      "Iteration 905/1000: Loss = 4513.303531234908\n",
      "gradient in iteration905: [-39310986.10969457    217076.24960072], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.543818151011449e-13\n",
      "Iteration 906/1000: Loss = 4513.675585754506\n",
      "gradient in iteration906: [5178912.20770252  -13268.41288574], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9309074181885417e-12\n",
      "Iteration 907/1000: Loss = 4513.303924018443\n",
      "gradient in iteration907: [-3.86546214e+07  1.41404629e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.587012788737347e-13\n",
      "Iteration 908/1000: Loss = 4513.6744379986085\n",
      "gradient in iteration908: [5179417.73191524  -13269.02938672], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307189567623884e-12\n",
      "Iteration 909/1000: Loss = 4513.303632612079\n",
      "gradient in iteration909: [-39149050.18232276     89889.08657557], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.55434038716867e-13\n",
      "Iteration 910/1000: Loss = 4513.67475282103\n",
      "gradient in iteration910: [5179478.8828829   -13269.08386545], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930696161933957e-12\n",
      "Iteration 911/1000: Loss = 4513.303597342573\n",
      "gradient in iteration911: [-39205856.45764732    134156.20660453], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5506393440996865e-13\n",
      "Iteration 912/1000: Loss = 4513.6747915066\n",
      "gradient in iteration912: [5179280.95309799  -13268.83902948], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.930769944816085e-12\n",
      "Iteration 913/1000: Loss = 4513.303711453744\n",
      "gradient in iteration913: [-3.90199915e+07  1.44254870e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5627888712578873e-13\n",
      "Iteration 914/1000: Loss = 4513.674666797048\n",
      "gradient in iteration914: [5179785.54848924  -13269.40441686], params: [0.08009705 0.04001708]\n",
      "learning_rate: 1.9305818564084484e-12\n",
      "Iteration 915/1000: Loss = 4513.303420565796\n",
      "gradient in iteration915: [-39483347.95609298    347310.80081656], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.532713287414327e-13\n",
      "Iteration 916/1000: Loss = 4513.67848349523\n",
      "gradient in iteration916: [5178350.54247371  -13267.72398897], params: [0.08009705 0.04001701]\n",
      "learning_rate: 1.931116852360285e-12\n",
      "Iteration 917/1000: Loss = 4513.304247811063\n",
      "gradient in iteration917: [-3.80420569e+07  1.37080514e+04], params: [0.08008705 0.04001704]\n",
      "learning_rate: 2.6286696400626846e-13\n",
      "Iteration 918/1000: Loss = 4513.674098215726\n",
      "gradient in iteration918: [5178857.33879666  -13268.32452833], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9309278757471167e-12\n",
      "Iteration 919/1000: Loss = 4513.303955674066\n",
      "gradient in iteration919: [-3.85980412e+07  1.40981118e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.5908050478792286e-13\n",
      "Iteration 920/1000: Loss = 4513.674404313518\n",
      "gradient in iteration920: [5179362.9351323   -13268.92274847], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.93073938344206e-12\n",
      "Iteration 921/1000: Loss = 4513.303664194224\n",
      "gradient in iteration921: [-39097735.91217345     50232.2889588 ], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5576928603905184e-13\n",
      "Iteration 922/1000: Loss = 4513.674718286193\n",
      "gradient in iteration922: [5179656.87963624  -13269.27508671], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930629814363743e-12\n",
      "Iteration 923/1000: Loss = 4513.303494734773\n",
      "gradient in iteration923: [-39368318.56399294    262830.44226147], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.540113564602732e-13\n",
      "Iteration 924/1000: Loss = 4513.6765417639335\n",
      "gradient in iteration924: [5178709.87955575  -13268.13862761], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.9309828572319698e-12\n",
      "Iteration 925/1000: Loss = 4513.304040660668\n",
      "gradient in iteration925: [-3.84429401e+07  1.39845027e+04], params: [0.08008705 0.04001705]\n",
      "learning_rate: 2.601257855882914e-13\n",
      "Iteration 926/1000: Loss = 4513.674314382161\n",
      "gradient in iteration926: [5179215.85528711  -13268.75494669], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.930794212755523e-12\n",
      "Iteration 927/1000: Loss = 4513.303748983061\n",
      "gradient in iteration927: [-3.89571683e+07  1.43751327e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5669216835069343e-13\n",
      "Iteration 928/1000: Loss = 4513.674626070441\n",
      "gradient in iteration928: [5179720.58473788  -13269.33975179], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306060696527036e-12\n",
      "Iteration 929/1000: Loss = 4513.3034580024205\n",
      "gradient in iteration929: [-39425525.76504567    308853.004708  ], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.5364278106510156e-13\n",
      "Iteration 930/1000: Loss = 4513.677503562637\n",
      "gradient in iteration930: [5178507.17324109  -13267.9041144 ], params: [0.08009705 0.04001702]\n",
      "learning_rate: 1.9310584431886135e-12\n",
      "Iteration 931/1000: Loss = 4513.304157530449\n",
      "gradient in iteration931: [-3.82211499e+07  1.38285103e+04], params: [0.08008705 0.04001705]\n",
      "learning_rate: 2.6163524694769306e-13\n",
      "Iteration 932/1000: Loss = 4513.674191894964\n",
      "gradient in iteration932: [5179013.5772535   -13268.51643171], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9308696242698665e-12\n",
      "Iteration 933/1000: Loss = 4513.303865589703\n",
      "gradient in iteration933: [-3.87574784e+07  1.42187422e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.5801472150515795e-13\n",
      "Iteration 934/1000: Loss = 4513.67450043426\n",
      "gradient in iteration934: [5179518.80224246  -13269.09473391], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930681281757397e-12\n",
      "Iteration 935/1000: Loss = 4513.3035743239325\n",
      "gradient in iteration935: [-39242655.67356997    163035.8471175 ], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5482475200410624e-13\n",
      "Iteration 936/1000: Loss = 4513.674816820157\n",
      "gradient in iteration936: [5179152.20666509  -13268.64753537], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.930817941038867e-12\n",
      "Iteration 937/1000: Loss = 4513.303785660145\n",
      "gradient in iteration937: [-3.88951125e+07  1.43259812e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.57101711972682e-13\n",
      "Iteration 938/1000: Loss = 4513.674586404797\n",
      "gradient in iteration938: [5179657.10791941  -13269.26767433], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306297292750423e-12\n",
      "Iteration 939/1000: Loss = 4513.303494590279\n",
      "gradient in iteration939: [-39368543.41827711    263011.01211175], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.540099056689365e-13\n",
      "Iteration 940/1000: Loss = 4513.67654553798\n",
      "gradient in iteration940: [5178709.08347504  -13268.1271225 ], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.9309831540662164e-12\n",
      "Iteration 941/1000: Loss = 4513.304041119937\n",
      "gradient in iteration941: [-3.84420885e+07  1.39838717e+04], params: [0.08008705 0.04001705]\n",
      "learning_rate: 2.60131548111775e-13\n",
      "Iteration 942/1000: Loss = 4513.674313896623\n",
      "gradient in iteration942: [5179215.03087564  -13268.73320977], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.93079452009339e-12\n",
      "Iteration 943/1000: Loss = 4513.303749441368\n",
      "gradient in iteration943: [-3.89563962e+07  1.43744905e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.566972557334538e-13\n",
      "Iteration 944/1000: Loss = 4513.674625573054\n",
      "gradient in iteration944: [5179719.7914766   -13269.34238932], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.93060636532025e-12\n",
      "Iteration 945/1000: Loss = 4513.303458460257\n",
      "gradient in iteration945: [-39424816.17245903    308279.3759404 ], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.536473462870753e-13\n",
      "Iteration 946/1000: Loss = 4513.677491576691\n",
      "gradient in iteration946: [5178509.67107734  -13267.921031  ], params: [0.08009705 0.04001702]\n",
      "learning_rate: 1.931057511749245e-12\n",
      "Iteration 947/1000: Loss = 4513.304156076889\n",
      "gradient in iteration947: [-3.82239765e+07  1.38305656e+04], params: [0.08008705 0.04001705]\n",
      "learning_rate: 2.6161589956001656e-13\n",
      "Iteration 948/1000: Loss = 4513.67419341281\n",
      "gradient in iteration948: [5179016.1131521   -13268.52830061], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9308686788220295e-12\n",
      "Iteration 949/1000: Loss = 4513.303864138964\n",
      "gradient in iteration949: [-3.87600073e+07  1.42206603e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.579978873289143e-13\n",
      "Iteration 950/1000: Loss = 4513.6745019913105\n",
      "gradient in iteration950: [5179521.35987798  -13269.11378783], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9306803283914985e-12\n",
      "Iteration 951/1000: Loss = 4513.303572876416\n",
      "gradient in iteration951: [-39244963.87389809    164852.53909195], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5480976443581394e-13\n",
      "Iteration 952/1000: Loss = 4513.674818416036\n",
      "gradient in iteration952: [5179144.15190717  -13268.69542027], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.930820943903947e-12\n",
      "Iteration 953/1000: Loss = 4513.303790321756\n",
      "gradient in iteration953: [-3.88871800e+07  1.43196189e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.5715415719106617e-13\n",
      "Iteration 954/1000: Loss = 4513.674581375005\n",
      "gradient in iteration954: [5179649.04429391  -13269.27463197], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306327348599737e-12\n",
      "Iteration 955/1000: Loss = 4513.3034992383455\n",
      "gradient in iteration955: [-39361269.86309318    257185.44725351], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.540568440698716e-13\n",
      "Iteration 956/1000: Loss = 4513.676423813135\n",
      "gradient in iteration956: [5178734.81103379  -13268.19224232], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.930973561089485e-12\n",
      "Iteration 957/1000: Loss = 4513.304026294285\n",
      "gradient in iteration957: [-3.84695040e+07  1.40038669e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.599461642652687e-13\n",
      "Iteration 958/1000: Loss = 4513.674329533026\n",
      "gradient in iteration958: [5179240.73510585  -13268.75271843], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9307849376875176e-12\n",
      "Iteration 959/1000: Loss = 4513.303734649122\n",
      "gradient in iteration959: [-3.89812398e+07  1.43942283e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.56533657143591e-13\n",
      "Iteration 960/1000: Loss = 4513.674641609189\n",
      "gradient in iteration960: [5179745.41799049  -13269.362762  ], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.93059681374834e-12\n",
      "Iteration 961/1000: Loss = 4513.303443704548\n",
      "gradient in iteration961: [-39447665.92962878    326760.9356044 ], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.5350042301207716e-13\n",
      "Iteration 962/1000: Loss = 4513.677877853351\n",
      "gradient in iteration962: [5178428.4903988  -13267.7838792], params: [0.08009705 0.04001702]\n",
      "learning_rate: 1.9310877843617534e-12\n",
      "Iteration 963/1000: Loss = 4513.304202884234\n",
      "gradient in iteration963: [-3.81320944e+07  1.37679631e+04], params: [0.08008705 0.04001704]\n",
      "learning_rate: 2.622462822731387e-13\n",
      "Iteration 964/1000: Loss = 4513.674144731574\n",
      "gradient in iteration964: [5178935.09850182  -13268.46354459], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9308988836127386e-12\n",
      "Iteration 965/1000: Loss = 4513.303910844347\n",
      "gradient in iteration965: [-3.86779886e+07  1.41582424e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.5854498560012525e-13\n",
      "Iteration 966/1000: Loss = 4513.674452047194\n",
      "gradient in iteration966: [5179440.5297187   -13269.03848167], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307104585180174e-12\n",
      "Iteration 967/1000: Loss = 4513.303619469456\n",
      "gradient in iteration967: [-39170278.15405163    106386.78045325], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5529560859055674e-13\n",
      "Iteration 968/1000: Loss = 4513.674767222128\n",
      "gradient in iteration968: [5179405.05892511  -13268.97772742], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9307236808536678e-12\n",
      "Iteration 969/1000: Loss = 4513.303639922333\n",
      "gradient in iteration969: [-39137209.47316506     80709.98997027], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.5551131863033396e-13\n",
      "Iteration 970/1000: Loss = 4513.674744817621\n",
      "gradient in iteration970: [5179520.03278879  -13269.13502453], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930680823067643e-12\n",
      "Iteration 971/1000: Loss = 4513.303573624522\n",
      "gradient in iteration971: [-39243772.04289158    163914.43741668], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.548175029930985e-13\n",
      "Iteration 972/1000: Loss = 4513.674817593038\n",
      "gradient in iteration972: [5179148.28242283  -13268.68691649], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9308194040202215e-12\n",
      "Iteration 973/1000: Loss = 4513.303787914815\n",
      "gradient in iteration973: [-3.88912778e+07  1.43228287e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.57127061872048e-13\n",
      "Iteration 974/1000: Loss = 4513.674583973535\n",
      "gradient in iteration974: [5179653.24625038  -13269.28827439], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.9306311686480422e-12\n",
      "Iteration 975/1000: Loss = 4513.303496837867\n",
      "gradient in iteration975: [-39365028.47554948    260195.04584838], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.5403258646723013e-13\n",
      "Iteration 976/1000: Loss = 4513.676486698401\n",
      "gradient in iteration976: [5178721.48534479  -13268.18482994], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.9309785297971494e-12\n",
      "Iteration 977/1000: Loss = 4513.304033954659\n",
      "gradient in iteration977: [-3.84553591e+07  1.39934701e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.6004177909830263e-13\n",
      "Iteration 978/1000: Loss = 4513.674321452358\n",
      "gradient in iteration978: [5179227.45539181  -13268.78004875], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.9307898882852784e-12\n",
      "Iteration 979/1000: Loss = 4513.303742291469\n",
      "gradient in iteration979: [-3.89684197e+07  1.43840807e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.56618053386651e-13\n",
      "Iteration 980/1000: Loss = 4513.674633322486\n",
      "gradient in iteration980: [5179732.18284169  -13269.37162958], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930601746770975e-12\n",
      "Iteration 981/1000: Loss = 4513.303451327281\n",
      "gradient in iteration981: [-39435872.54416343    317215.18271297], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.535762328778501e-13\n",
      "Iteration 982/1000: Loss = 4513.677678331688\n",
      "gradient in iteration982: [5178470.42874564  -13267.88378719], params: [0.08009705 0.04001702]\n",
      "learning_rate: 1.93107214525936e-12\n",
      "Iteration 983/1000: Loss = 4513.304178717134\n",
      "gradient in iteration983: [-3.81797718e+07  1.38002466e+04], params: [0.08008705 0.04001704]\n",
      "learning_rate: 2.61918799557439e-13\n",
      "Iteration 984/1000: Loss = 4513.674169837895\n",
      "gradient in iteration984: [5178976.90128848  -13268.45863332], params: [0.08009705 0.04001704]\n",
      "learning_rate: 1.9308832981108873e-12\n",
      "Iteration 985/1000: Loss = 4513.303886728484\n",
      "gradient in iteration985: [-3.87204923e+07  1.41904810e+04], params: [0.08008705 0.04001707]\n",
      "learning_rate: 2.582611793834786e-13\n",
      "Iteration 986/1000: Loss = 4513.674477806937\n",
      "gradient in iteration986: [5179482.25310647  -13269.0753162 ], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.9306949056543927e-12\n",
      "Iteration 987/1000: Loss = 4513.30359541309\n",
      "gradient in iteration987: [-39208949.03621957    136577.14607689], params: [0.08008705 0.04001709]\n",
      "learning_rate: 2.5504381641962464e-13\n",
      "Iteration 988/1000: Loss = 4513.67479362689\n",
      "gradient in iteration988: [5179270.15630448  -13268.83120782], params: [0.08009705 0.04001705]\n",
      "learning_rate: 1.930773969731521e-12\n",
      "Iteration 989/1000: Loss = 4513.303717681011\n",
      "gradient in iteration989: [-3.90096124e+07  1.44170947e+04], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.563470743962953e-13\n",
      "Iteration 990/1000: Loss = 4513.674660030385\n",
      "gradient in iteration990: [5179774.77206841  -13269.44016   ], params: [0.08009705 0.04001708]\n",
      "learning_rate: 1.930585872946509e-12\n",
      "Iteration 991/1000: Loss = 4513.303426778118\n",
      "gradient in iteration991: [-39473787.70867544    347328.19835881], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.533326691069534e-13\n",
      "Iteration 992/1000: Loss = 4513.678320942235\n",
      "gradient in iteration992: [5178339.17269854  -13267.7062993 ], params: [0.08009705 0.04001701]\n",
      "learning_rate: 1.9311210924001313e-12\n",
      "Iteration 993/1000: Loss = 4513.304254362455\n",
      "gradient in iteration993: [-3.80287683e+07  1.36994091e+04], params: [0.08008705 0.04001704]\n",
      "learning_rate: 2.6295881933138725e-13\n",
      "Iteration 994/1000: Loss = 4513.6740914506345\n",
      "gradient in iteration994: [5178845.9555155   -13268.30083599], params: [0.08009705 0.04001703]\n",
      "learning_rate: 1.9309321199928622e-12\n",
      "Iteration 995/1000: Loss = 4513.303962211619\n",
      "gradient in iteration995: [-3.85862792e+07  1.40893490e+04], params: [0.08008705 0.04001706]\n",
      "learning_rate: 2.591594788208842e-13\n",
      "Iteration 996/1000: Loss = 4513.674397370476\n",
      "gradient in iteration996: [5179351.61860805  -13268.89978372], params: [0.08009705 0.04001706]\n",
      "learning_rate: 1.930743601973773e-12\n",
      "Iteration 997/1000: Loss = 4513.303670715786\n",
      "gradient in iteration997: [-39087084.51322127     42040.38314128], params: [0.08008705 0.04001708]\n",
      "learning_rate: 2.558389842715817e-13\n",
      "Iteration 998/1000: Loss = 4513.674711167743\n",
      "gradient in iteration998: [5179693.74454879  -13269.31792391], params: [0.08009705 0.04001707]\n",
      "learning_rate: 1.930616073686635e-12\n",
      "Iteration 999/1000: Loss = 4513.303473493804\n",
      "gradient in iteration999: [-39401458.22666636    289446.71657882], params: [0.08008705 0.0400171 ]\n",
      "learning_rate: 2.5379771333519173e-13\n",
      "Iteration 1000/1000: Loss = 4513.677097983018\n"
     ]
    }
   ],
   "source": [
    "mean_speed = 0.01 # m / frame\n",
    "std_dev_speed = 0.002\n",
    "# c_values = np.random.normal(mean_speed, std_dev_speed, num_pedestrians)  # Random walking speeds normally distributed\n",
    "# c_values = np.clip(c_values, 0.001, 0.1)\n",
    "c_values = 0.08\n",
    "s_value = 0.04  # Safety distance\n",
    "\n",
    "# params = np.concatenate([\n",
    "#     [c_values],  # initial speeds\n",
    "#     [s_value] # initial safe distance\n",
    "# ])\n",
    "params = [c_values, s_value]\n",
    "\n",
    "# params = initial_guess.copy()\n",
    "learning_rate = 0.0000001\n",
    "learning_rate_max = 0.1\n",
    "\n",
    "epsilon = 1e-8\n",
    "\n",
    "num_iterations = 1000\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    predicted_positions = compute_predicted_positions(params, t_eval)\n",
    "    loss = loss_function(interpolated_real_positions, predicted_positions, t_eval)\n",
    "    grad = compute_gradient(params, interpolated_real_positions, epsilon, t_eval)\n",
    "    print(f'gradient in iteration{i}: {grad}, params: {params}')\n",
    "    max_grad_component = np.max(np.abs(grad))\n",
    "    learning_rate = np.min([learning_rate_max, 0.00001 / max_grad_component])\n",
    "    print(f'learning_rate: {learning_rate}')\n",
    "    params -= learning_rate * grad\n",
    "    loss_history.append(loss)\n",
    "\n",
    "    print(f'Iteration {i+1}/{num_iterations}: Loss = {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pdips.github.io/CBXpy/examples/low_level.html \\\n",
    "Use gradient-free method cbx to optimize loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize loss function by cbx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(params_batch):\n",
    "    losses = []\n",
    "    for params in params_batch:\n",
    "        if (min(params) < 0):\n",
    "            losses.append(np.inf)\n",
    "            continue\n",
    "        predicted_positions = compute_predicted_positions(params, t_eval)\n",
    "        loss = loss_function(interpolated_real_positions, predicted_positions, t_eval)\n",
    "        losses.append(loss)\n",
    "    return np.array(losses)\n",
    "\n",
    "from cbx.utils import init_particles\n",
    "\n",
    "N = 50  # Number of particles\n",
    "d = 2   # Dimension of the parameter space\n",
    "x_min = np.array([0, 0])  # Lower bounds for c_values and s_value\n",
    "x_max = np.array([0.2, 0.2])    # Upper bounds for c_values and s_value\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cbx.dynamics import CBO, CBXDynamic\n",
    "\n",
    "cbo_params = {\n",
    "    'alpha': 40.0,   # alpha parameter\n",
    "    'dt': 0.1,       \n",
    "    'sigma': 1.,    # Noise intensity\n",
    "    'lamda': 1.0,   \n",
    "    'max_it': 100,  # Maximum number of iterations\n",
    "    'batch_args': {\n",
    "        'batch_size': 200,\n",
    "        'batch_partial': False},\n",
    "    'N': N,          # Number of particles\n",
    "    'd': d,           # Dimension of the parameter space\n",
    "    'track_args': {\n",
    "        'names': [\n",
    "            'energy', \n",
    "            'x', \n",
    "            'consensus', \n",
    "            'drift'\n",
    "        ]\n",
    "    },  # Track energy, positions, consensus, and drift\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100: Loss = 4260.892531433349\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "from cbx.plotting import PlotDynamic\n",
    "from cbx.scheduler import effective_sample_size\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "particles = init_particles(shape=(N, d), x_min=x_min, x_max=x_max)\n",
    "cbo = CBO(objective_function, x=particles, noise='isotropic', f_dim='2D', **cbo_params)\n",
    "\n",
    "sched = effective_sample_size(maximum=1e12)\n",
    "plotter = PlotDynamic(\n",
    "    cbo, ax=ax[0],\n",
    "    objective_args={'x_min': 0, 'x_max': 0.2},\n",
    "    particle_args={'color': 'xkcd:white'},\n",
    "    plot_consensus=True,\n",
    "    plot_drift=True\n",
    ")\n",
    "plotter.init_plot()\n",
    "\n",
    "while not cbo.terminate():\n",
    "    display.clear_output(wait=True)\n",
    "    cbo.step()\n",
    "    sched.update(cbo)\n",
    "    current_loss = cbo.history['energy'][-1][0]\n",
    "    print(f'Iteration {cbo.it}: Loss = {current_loss}')\n",
    "\n",
    "    # Update loss plot\n",
    "    ax[1].clear()\n",
    "    ax[1].plot([e[0] for e in cbo.history['energy'][-30:]])\n",
    "    ax[1].set_title('Loss over Iterations')\n",
    "    ax[1].set_xlabel('Iteration')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "\n",
    "    ax[0].set_title(f'Iteration: {cbo.it}')\n",
    "    # ax[0].set_xlabel('c value')\n",
    "    # ax[0].set_ylabel('s value')\n",
    "    plotter.update(wait=0.2)\n",
    "    display.display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters: [[0.08873866 0.04567424]]\n",
      "Final loss: [4260.89253143]\n"
     ]
    }
   ],
   "source": [
    "print(f'Final parameters: {cbo.best_particle}')\n",
    "print(f'Final loss: {objective_function(cbo.best_particle)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task:\\\n",
    "- Exclude ending from the dataset\n",
    "- Add accelaration term to simulate the beginning behaviour\n",
    "- Add current result to report and share"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_laptop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
